{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import WD_LSTM\n",
    "from data import Corpus\n",
    "from randomize_bptt import get_bptt_sequence_lengths\n",
    "from helpers import Config, repackage_hidden, batchify, get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "DATA = '/floyd/input/ptb/'\n",
    "CUDA = True\n",
    "LOG_INTERVAL = 100\n",
    "LR_ANNEALING_RATE = 0.25\n",
    "CONFIG_NAME = 'language_model_base'\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
    "args = Config(CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "train_data = batchify(corpus.train, args.batch_size, device)\n",
    "valid_data = batchify(corpus.valid, args.eval_batch_size, device)\n",
    "test_data = batchify(corpus.test, args.test_batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WD_LSTM(\n",
       "  (variational_dropout): VariationalDropout()\n",
       "  (encoder): Embedding(10000, 400)\n",
       "  (rnns): ModuleList(\n",
       "    (0): WeightDrop(\n",
       "      (module): LSTM(400, 1150)\n",
       "    )\n",
       "    (1): WeightDrop(\n",
       "      (module): LSTM(1150, 1150)\n",
       "    )\n",
       "    (2): WeightDrop(\n",
       "      (module): LSTM(1150, 400)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=400, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WD_LSTM(\n",
    "    ntoken=ntokens, \n",
    "    ninp=args.emsize,\n",
    "    nhid=args.nhid, \n",
    "    nlayers=args.nlayers, \n",
    "    dropout=args.dropout,\n",
    "    dropout_h=args.dropout_h,\n",
    "    dropout_i=args.dropout_i,\n",
    "    dropout_e=args.dropout_e,\n",
    "    weight_drop=args.weight_drop, \n",
    "    weight_tying=args.weight_tying\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = args.lr\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=args.weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source, batch_size):\n",
    "    model.eval()  # disable dropout\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, args.bptt_seq_len):\n",
    "            data, targets = get_batch(data_source, i, args.bptt_seq_len)\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args.batch_size)\n",
    "    for batch, (i, seq_len, lr_scale) in enumerate(get_bptt_sequence_lengths(\n",
    "        train_data.size(0), \n",
    "        args.bptt_seq_len, \n",
    "        args.bptt_random_scaling, \n",
    "        args.bptt_p, \n",
    "        args.bptt_s, \n",
    "        args.bptt_min_len, \n",
    "        args.bptt_max_len\n",
    "    )):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr * lr_scale\n",
    "        data, targets = get_batch(train_data, i, seq_len)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
    "            cur_loss = total_loss / LOG_INTERVAL\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:3.2E} | ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args.bptt_seq_len, lr,\n",
    "                elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py:477: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/  331 batches | lr 2.00E-03 | ms/batch 376.23 | loss  6.84 | ppl   932.34\n",
      "| epoch   1 |   200/  331 batches | lr 2.00E-03 | ms/batch 368.09 | loss  6.43 | ppl   621.35\n",
      "| epoch   1 |   300/  331 batches | lr 2.00E-03 | ms/batch 368.85 | loss  6.11 | ppl   448.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 136.47s | valid loss  5.79 | valid ppl   326.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   100/  331 batches | lr 2.00E-03 | ms/batch 376.25 | loss  5.90 | ppl   365.61\n",
      "| epoch   2 |   200/  331 batches | lr 2.00E-03 | ms/batch 360.36 | loss  5.77 | ppl   320.50\n",
      "| epoch   2 |   300/  331 batches | lr 2.00E-03 | ms/batch 370.66 | loss  5.64 | ppl   282.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 137.29s | valid loss  5.42 | valid ppl   226.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   100/  331 batches | lr 2.00E-03 | ms/batch 371.81 | loss  5.60 | ppl   270.45\n",
      "| epoch   3 |   200/  331 batches | lr 2.00E-03 | ms/batch 379.38 | loss  5.53 | ppl   251.83\n",
      "| epoch   3 |   300/  331 batches | lr 2.00E-03 | ms/batch 373.89 | loss  5.44 | ppl   230.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 137.47s | valid loss  5.23 | valid ppl   187.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   100/  331 batches | lr 2.00E-03 | ms/batch 372.52 | loss  5.42 | ppl   226.05\n",
      "| epoch   4 |   200/  331 batches | lr 2.00E-03 | ms/batch 365.42 | loss  5.38 | ppl   217.43\n",
      "| epoch   4 |   300/  331 batches | lr 2.00E-03 | ms/batch 373.82 | loss  5.30 | ppl   200.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 137.73s | valid loss  5.12 | valid ppl   167.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   100/  331 batches | lr 2.00E-03 | ms/batch 381.10 | loss  5.31 | ppl   202.75\n",
      "| epoch   5 |   200/  331 batches | lr 2.00E-03 | ms/batch 368.43 | loss  5.28 | ppl   196.46\n",
      "| epoch   5 |   300/  331 batches | lr 2.00E-03 | ms/batch 373.37 | loss  5.21 | ppl   182.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 137.80s | valid loss  5.03 | valid ppl   153.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   100/  331 batches | lr 2.00E-03 | ms/batch 378.99 | loss  5.23 | ppl   186.01\n",
      "| epoch   6 |   200/  331 batches | lr 2.00E-03 | ms/batch 378.29 | loss  5.22 | ppl   185.37\n",
      "| epoch   6 |   300/  331 batches | lr 2.00E-03 | ms/batch 371.92 | loss  5.13 | ppl   169.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 137.67s | valid loss  4.98 | valid ppl   145.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   100/  331 batches | lr 2.00E-03 | ms/batch 379.73 | loss  5.17 | ppl   175.32\n",
      "| epoch   7 |   200/  331 batches | lr 2.00E-03 | ms/batch 371.30 | loss  5.15 | ppl   173.04\n",
      "| epoch   7 |   300/  331 batches | lr 2.00E-03 | ms/batch 367.87 | loss  5.09 | ppl   161.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 138.11s | valid loss  4.93 | valid ppl   138.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   100/  331 batches | lr 2.00E-03 | ms/batch 382.70 | loss  5.11 | ppl   165.28\n",
      "| epoch   8 |   200/  331 batches | lr 2.00E-03 | ms/batch 369.79 | loss  5.10 | ppl   163.35\n",
      "| epoch   8 |   300/  331 batches | lr 2.00E-03 | ms/batch 372.11 | loss  5.04 | ppl   153.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 137.86s | valid loss  4.88 | valid ppl   131.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   100/  331 batches | lr 2.00E-03 | ms/batch 376.07 | loss  5.07 | ppl   158.47\n",
      "| epoch   9 |   200/  331 batches | lr 2.00E-03 | ms/batch 384.03 | loss  5.06 | ppl   158.30\n",
      "| epoch   9 |   300/  331 batches | lr 2.00E-03 | ms/batch 369.05 | loss  4.99 | ppl   147.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 137.85s | valid loss  4.85 | valid ppl   127.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   100/  331 batches | lr 2.00E-03 | ms/batch 382.72 | loss  5.03 | ppl   153.14\n",
      "| epoch  10 |   200/  331 batches | lr 2.00E-03 | ms/batch 369.29 | loss  5.02 | ppl   151.61\n",
      "| epoch  10 |   300/  331 batches | lr 2.00E-03 | ms/batch 380.83 | loss  4.95 | ppl   141.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 137.84s | valid loss  4.82 | valid ppl   123.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   100/  331 batches | lr 2.00E-03 | ms/batch 379.77 | loss  4.99 | ppl   147.53\n",
      "| epoch  11 |   200/  331 batches | lr 2.00E-03 | ms/batch 373.50 | loss  4.99 | ppl   147.36\n",
      "| epoch  11 |   300/  331 batches | lr 2.00E-03 | ms/batch 365.77 | loss  4.92 | ppl   137.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 137.85s | valid loss  4.79 | valid ppl   120.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   100/  331 batches | lr 2.00E-03 | ms/batch 373.13 | loss  4.97 | ppl   143.31\n",
      "| epoch  12 |   200/  331 batches | lr 2.00E-03 | ms/batch 379.67 | loss  4.96 | ppl   143.05\n",
      "| epoch  12 |   300/  331 batches | lr 2.00E-03 | ms/batch 381.71 | loss  4.89 | ppl   133.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 137.67s | valid loss  4.77 | valid ppl   117.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   100/  331 batches | lr 2.00E-03 | ms/batch 377.10 | loss  4.94 | ppl   140.46\n",
      "| epoch  13 |   200/  331 batches | lr 2.00E-03 | ms/batch 372.90 | loss  4.94 | ppl   140.19\n",
      "| epoch  13 |   300/  331 batches | lr 2.00E-03 | ms/batch 381.37 | loss  4.89 | ppl   132.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 137.81s | valid loss  4.75 | valid ppl   115.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   100/  331 batches | lr 2.00E-03 | ms/batch 378.10 | loss  4.93 | ppl   138.12\n",
      "| epoch  14 |   200/  331 batches | lr 2.00E-03 | ms/batch 375.97 | loss  4.93 | ppl   138.59\n",
      "| epoch  14 |   300/  331 batches | lr 2.00E-03 | ms/batch 374.26 | loss  4.85 | ppl   127.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 137.74s | valid loss  4.74 | valid ppl   114.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   100/  331 batches | lr 2.00E-03 | ms/batch 380.07 | loss  4.91 | ppl   135.30\n",
      "| epoch  15 |   200/  331 batches | lr 2.00E-03 | ms/batch 370.49 | loss  4.90 | ppl   134.93\n",
      "| epoch  15 |   300/  331 batches | lr 2.00E-03 | ms/batch 375.85 | loss  4.85 | ppl   127.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 137.93s | valid loss  4.72 | valid ppl   112.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   100/  331 batches | lr 2.00E-03 | ms/batch 368.80 | loss  4.89 | ppl   132.98\n",
      "| epoch  16 |   200/  331 batches | lr 2.00E-03 | ms/batch 371.95 | loss  4.90 | ppl   134.22\n",
      "| epoch  16 |   300/  331 batches | lr 2.00E-03 | ms/batch 363.88 | loss  4.83 | ppl   125.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 138.13s | valid loss  4.71 | valid ppl   111.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   100/  331 batches | lr 2.00E-03 | ms/batch 386.17 | loss  4.89 | ppl   132.47\n",
      "| epoch  17 |   200/  331 batches | lr 2.00E-03 | ms/batch 373.11 | loss  4.89 | ppl   132.71\n",
      "| epoch  17 |   300/  331 batches | lr 2.00E-03 | ms/batch 373.91 | loss  4.81 | ppl   123.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 137.93s | valid loss  4.70 | valid ppl   110.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   100/  331 batches | lr 2.00E-03 | ms/batch 376.73 | loss  4.87 | ppl   129.86\n",
      "| epoch  18 |   200/  331 batches | lr 2.00E-03 | ms/batch 380.50 | loss  4.87 | ppl   130.75\n",
      "| epoch  18 |   300/  331 batches | lr 2.00E-03 | ms/batch 376.40 | loss  4.81 | ppl   122.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 137.89s | valid loss  4.69 | valid ppl   109.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   100/  331 batches | lr 2.00E-03 | ms/batch 376.84 | loss  4.86 | ppl   128.74\n",
      "| epoch  19 |   200/  331 batches | lr 2.00E-03 | ms/batch 371.05 | loss  4.85 | ppl   127.67\n",
      "| epoch  19 |   300/  331 batches | lr 2.00E-03 | ms/batch 376.71 | loss  4.79 | ppl   120.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 138.01s | valid loss  4.69 | valid ppl   108.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   100/  331 batches | lr 2.00E-03 | ms/batch 383.86 | loss  4.85 | ppl   127.20\n",
      "| epoch  20 |   200/  331 batches | lr 2.00E-03 | ms/batch 372.13 | loss  4.86 | ppl   128.41\n",
      "| epoch  20 |   300/  331 batches | lr 2.00E-03 | ms/batch 379.70 | loss  4.79 | ppl   119.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 138.24s | valid loss  4.68 | valid ppl   107.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   100/  331 batches | lr 2.00E-03 | ms/batch 382.11 | loss  4.84 | ppl   126.48\n",
      "| epoch  21 |   200/  331 batches | lr 2.00E-03 | ms/batch 373.73 | loss  4.84 | ppl   126.65\n",
      "| epoch  21 |   300/  331 batches | lr 2.00E-03 | ms/batch 370.79 | loss  4.77 | ppl   117.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 138.65s | valid loss  4.67 | valid ppl   107.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   100/  331 batches | lr 2.00E-03 | ms/batch 375.88 | loss  4.82 | ppl   124.58\n",
      "| epoch  22 |   200/  331 batches | lr 2.00E-03 | ms/batch 380.06 | loss  4.84 | ppl   126.34\n",
      "| epoch  22 |   300/  331 batches | lr 2.00E-03 | ms/batch 381.63 | loss  4.78 | ppl   118.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 138.71s | valid loss  4.67 | valid ppl   106.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   100/  331 batches | lr 2.00E-03 | ms/batch 379.08 | loss  4.82 | ppl   124.46\n",
      "| epoch  23 |   200/  331 batches | lr 2.00E-03 | ms/batch 371.84 | loss  4.82 | ppl   124.57\n",
      "| epoch  23 |   300/  331 batches | lr 2.00E-03 | ms/batch 371.13 | loss  4.76 | ppl   116.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 138.79s | valid loss  4.66 | valid ppl   105.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   100/  331 batches | lr 2.00E-03 | ms/batch 378.32 | loss  4.82 | ppl   124.46\n",
      "| epoch  24 |   200/  331 batches | lr 2.00E-03 | ms/batch 374.12 | loss  4.82 | ppl   123.94\n",
      "| epoch  24 |   300/  331 batches | lr 2.00E-03 | ms/batch 377.38 | loss  4.75 | ppl   115.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 138.80s | valid loss  4.65 | valid ppl   104.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |   100/  331 batches | lr 2.00E-03 | ms/batch 376.96 | loss  4.80 | ppl   121.83\n",
      "| epoch  25 |   200/  331 batches | lr 2.00E-03 | ms/batch 371.56 | loss  4.81 | ppl   122.48\n",
      "| epoch  25 |   300/  331 batches | lr 2.00E-03 | ms/batch 374.77 | loss  4.74 | ppl   114.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 139.00s | valid loss  4.64 | valid ppl   103.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   100/  331 batches | lr 2.00E-03 | ms/batch 379.18 | loss  4.81 | ppl   122.21\n",
      "| epoch  26 |   200/  331 batches | lr 2.00E-03 | ms/batch 376.98 | loss  4.82 | ppl   123.54\n",
      "| epoch  26 |   300/  331 batches | lr 2.00E-03 | ms/batch 378.63 | loss  4.73 | ppl   112.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 138.85s | valid loss  4.64 | valid ppl   103.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   100/  331 batches | lr 2.00E-03 | ms/batch 378.64 | loss  4.78 | ppl   119.65\n",
      "| epoch  27 |   200/  331 batches | lr 2.00E-03 | ms/batch 370.21 | loss  4.80 | ppl   121.61\n",
      "| epoch  27 |   300/  331 batches | lr 2.00E-03 | ms/batch 374.57 | loss  4.74 | ppl   114.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 138.93s | valid loss  4.63 | valid ppl   102.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   100/  331 batches | lr 2.00E-03 | ms/batch 379.41 | loss  4.79 | ppl   120.42\n",
      "| epoch  28 |   200/  331 batches | lr 2.00E-03 | ms/batch 384.53 | loss  4.79 | ppl   120.11\n",
      "| epoch  28 |   300/  331 batches | lr 2.00E-03 | ms/batch 372.97 | loss  4.74 | ppl   114.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 138.74s | valid loss  4.63 | valid ppl   102.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |   100/  331 batches | lr 5.00E-04 | ms/batch 377.93 | loss  4.75 | ppl   116.04\n",
      "| epoch  29 |   200/  331 batches | lr 5.00E-04 | ms/batch 377.36 | loss  4.70 | ppl   109.85\n",
      "| epoch  29 |   300/  331 batches | lr 5.00E-04 | ms/batch 373.97 | loss  4.58 | ppl    97.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 139.01s | valid loss  4.57 | valid ppl    96.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |   100/  331 batches | lr 5.00E-04 | ms/batch 381.64 | loss  4.69 | ppl   109.29\n",
      "| epoch  30 |   200/  331 batches | lr 5.00E-04 | ms/batch 373.71 | loss  4.64 | ppl   104.04\n",
      "| epoch  30 |   300/  331 batches | lr 5.00E-04 | ms/batch 378.10 | loss  4.56 | ppl    95.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 138.79s | valid loss  4.56 | valid ppl    95.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |   100/  331 batches | lr 5.00E-04 | ms/batch 373.75 | loss  4.66 | ppl   105.67\n",
      "| epoch  31 |   200/  331 batches | lr 5.00E-04 | ms/batch 371.32 | loss  4.66 | ppl   105.78\n",
      "| epoch  31 |   300/  331 batches | lr 5.00E-04 | ms/batch 374.71 | loss  4.54 | ppl    93.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 138.92s | valid loss  4.55 | valid ppl    94.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |   100/  331 batches | lr 5.00E-04 | ms/batch 384.47 | loss  4.64 | ppl   103.55\n",
      "| epoch  32 |   200/  331 batches | lr 5.00E-04 | ms/batch 369.39 | loss  4.64 | ppl   103.41\n",
      "| epoch  32 |   300/  331 batches | lr 5.00E-04 | ms/batch 372.19 | loss  4.54 | ppl    93.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 138.85s | valid loss  4.55 | valid ppl    94.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |   100/  331 batches | lr 5.00E-04 | ms/batch 372.22 | loss  4.63 | ppl   102.03\n",
      "| epoch  33 |   200/  331 batches | lr 5.00E-04 | ms/batch 386.17 | loss  4.63 | ppl   102.17\n",
      "| epoch  33 |   300/  331 batches | lr 5.00E-04 | ms/batch 376.03 | loss  4.54 | ppl    94.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 138.29s | valid loss  4.54 | valid ppl    93.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |   100/  331 batches | lr 5.00E-04 | ms/batch 373.25 | loss  4.63 | ppl   102.02\n",
      "| epoch  34 |   200/  331 batches | lr 5.00E-04 | ms/batch 371.69 | loss  4.61 | ppl   100.78\n",
      "| epoch  34 |   300/  331 batches | lr 5.00E-04 | ms/batch 363.93 | loss  4.54 | ppl    93.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 138.26s | valid loss  4.54 | valid ppl    93.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |   100/  331 batches | lr 5.00E-04 | ms/batch 373.64 | loss  4.61 | ppl   100.39\n",
      "| epoch  35 |   200/  331 batches | lr 5.00E-04 | ms/batch 380.20 | loss  4.62 | ppl   101.54\n",
      "| epoch  35 |   300/  331 batches | lr 5.00E-04 | ms/batch 373.43 | loss  4.53 | ppl    92.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 137.98s | valid loss  4.53 | valid ppl    93.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |   100/  331 batches | lr 5.00E-04 | ms/batch 374.70 | loss  4.61 | ppl   100.97\n",
      "| epoch  36 |   200/  331 batches | lr 5.00E-04 | ms/batch 377.89 | loss  4.61 | ppl   100.86\n",
      "| epoch  36 |   300/  331 batches | lr 5.00E-04 | ms/batch 377.01 | loss  4.54 | ppl    93.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 138.14s | valid loss  4.54 | valid ppl    93.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |   100/  331 batches | lr 1.25E-04 | ms/batch 371.13 | loss  4.58 | ppl    97.82\n",
      "| epoch  37 |   200/  331 batches | lr 1.25E-04 | ms/batch 372.58 | loss  4.58 | ppl    97.03\n",
      "| epoch  37 |   300/  331 batches | lr 1.25E-04 | ms/batch 376.28 | loss  4.49 | ppl    88.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 138.11s | valid loss  4.52 | valid ppl    92.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |   100/  331 batches | lr 1.25E-04 | ms/batch 387.54 | loss  4.58 | ppl    97.14\n",
      "| epoch  38 |   200/  331 batches | lr 1.25E-04 | ms/batch 374.91 | loss  4.58 | ppl    97.26\n",
      "| epoch  38 |   300/  331 batches | lr 1.25E-04 | ms/batch 373.57 | loss  4.49 | ppl    88.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 138.09s | valid loss  4.52 | valid ppl    91.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |   100/  331 batches | lr 1.25E-04 | ms/batch 376.40 | loss  4.58 | ppl    97.63\n",
      "| epoch  39 |   200/  331 batches | lr 1.25E-04 | ms/batch 371.14 | loss  4.56 | ppl    95.63\n",
      "| epoch  39 |   300/  331 batches | lr 1.25E-04 | ms/batch 373.38 | loss  4.48 | ppl    88.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 138.28s | valid loss  4.52 | valid ppl    91.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |   100/  331 batches | lr 1.25E-04 | ms/batch 379.24 | loss  4.57 | ppl    96.71\n",
      "| epoch  40 |   200/  331 batches | lr 1.25E-04 | ms/batch 376.66 | loss  4.57 | ppl    96.68\n",
      "| epoch  40 |   300/  331 batches | lr 1.25E-04 | ms/batch 377.28 | loss  4.49 | ppl    88.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 138.83s | valid loss  4.52 | valid ppl    91.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |   100/  331 batches | lr 1.25E-04 | ms/batch 375.19 | loss  4.57 | ppl    96.94\n",
      "| epoch  41 |   200/  331 batches | lr 1.25E-04 | ms/batch 364.43 | loss  4.56 | ppl    95.92\n",
      "| epoch  41 |   300/  331 batches | lr 1.25E-04 | ms/batch 371.49 | loss  4.47 | ppl    87.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 139.01s | valid loss  4.52 | valid ppl    91.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |   100/  331 batches | lr 1.25E-04 | ms/batch 378.78 | loss  4.57 | ppl    96.76\n",
      "| epoch  42 |   200/  331 batches | lr 1.25E-04 | ms/batch 375.89 | loss  4.56 | ppl    95.66\n",
      "| epoch  42 |   300/  331 batches | lr 1.25E-04 | ms/batch 378.39 | loss  4.47 | ppl    87.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 138.99s | valid loss  4.52 | valid ppl    91.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |   100/  331 batches | lr 3.13E-05 | ms/batch 382.32 | loss  4.58 | ppl    97.79\n",
      "| epoch  43 |   200/  331 batches | lr 3.13E-05 | ms/batch 381.13 | loss  4.57 | ppl    96.19\n",
      "| epoch  43 |   300/  331 batches | lr 3.13E-05 | ms/batch 378.27 | loss  4.45 | ppl    85.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 138.84s | valid loss  4.51 | valid ppl    90.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |   100/  331 batches | lr 3.13E-05 | ms/batch 385.94 | loss  4.56 | ppl    95.53\n",
      "| epoch  44 |   200/  331 batches | lr 3.13E-05 | ms/batch 383.90 | loss  4.55 | ppl    95.04\n",
      "| epoch  44 |   300/  331 batches | lr 3.13E-05 | ms/batch 382.68 | loss  4.46 | ppl    86.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 138.73s | valid loss  4.51 | valid ppl    90.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |   100/  331 batches | lr 3.13E-05 | ms/batch 382.22 | loss  4.57 | ppl    96.27\n",
      "| epoch  45 |   200/  331 batches | lr 3.13E-05 | ms/batch 377.14 | loss  4.56 | ppl    95.41\n",
      "| epoch  45 |   300/  331 batches | lr 3.13E-05 | ms/batch 372.30 | loss  4.47 | ppl    87.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 138.80s | valid loss  4.51 | valid ppl    90.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |   100/  331 batches | lr 3.13E-05 | ms/batch 389.28 | loss  4.57 | ppl    96.64\n",
      "| epoch  46 |   200/  331 batches | lr 3.13E-05 | ms/batch 380.21 | loss  4.56 | ppl    95.20\n",
      "| epoch  46 |   300/  331 batches | lr 3.13E-05 | ms/batch 377.70 | loss  4.47 | ppl    87.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 139.04s | valid loss  4.51 | valid ppl    90.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |   100/  331 batches | lr 3.13E-05 | ms/batch 385.57 | loss  4.57 | ppl    96.31\n",
      "| epoch  47 |   200/  331 batches | lr 3.13E-05 | ms/batch 378.11 | loss  4.57 | ppl    96.47\n",
      "| epoch  47 |   300/  331 batches | lr 3.13E-05 | ms/batch 376.30 | loss  4.47 | ppl    87.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 138.97s | valid loss  4.51 | valid ppl    90.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |   100/  331 batches | lr 3.13E-05 | ms/batch 373.82 | loss  4.57 | ppl    96.20\n",
      "| epoch  48 |   200/  331 batches | lr 3.13E-05 | ms/batch 378.10 | loss  4.55 | ppl    94.75\n",
      "| epoch  48 |   300/  331 batches | lr 3.13E-05 | ms/batch 377.15 | loss  4.48 | ppl    87.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 138.83s | valid loss  4.51 | valid ppl    90.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |   100/  331 batches | lr 3.13E-05 | ms/batch 377.86 | loss  4.57 | ppl    96.43\n",
      "| epoch  49 |   200/  331 batches | lr 3.13E-05 | ms/batch 372.11 | loss  4.57 | ppl    96.40\n",
      "| epoch  49 |   300/  331 batches | lr 3.13E-05 | ms/batch 376.16 | loss  4.47 | ppl    87.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 138.94s | valid loss  4.51 | valid ppl    90.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |   100/  331 batches | lr 3.13E-05 | ms/batch 384.48 | loss  4.55 | ppl    94.53\n",
      "| epoch  50 |   200/  331 batches | lr 3.13E-05 | ms/batch 377.63 | loss  4.56 | ppl    95.25\n",
      "| epoch  50 |   300/  331 batches | lr 3.13E-05 | ms/batch 383.23 | loss  4.46 | ppl    86.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 138.94s | valid loss  4.50 | valid ppl    90.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 |   100/  331 batches | lr 3.13E-05 | ms/batch 377.07 | loss  4.56 | ppl    96.03\n",
      "| epoch  51 |   200/  331 batches | lr 3.13E-05 | ms/batch 376.62 | loss  4.56 | ppl    95.83\n",
      "| epoch  51 |   300/  331 batches | lr 3.13E-05 | ms/batch 378.26 | loss  4.47 | ppl    86.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 139.15s | valid loss  4.50 | valid ppl    90.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 |   100/  331 batches | lr 7.81E-06 | ms/batch 380.34 | loss  4.56 | ppl    95.26\n",
      "| epoch  52 |   200/  331 batches | lr 7.81E-06 | ms/batch 371.35 | loss  4.57 | ppl    96.13\n",
      "| epoch  52 |   300/  331 batches | lr 7.81E-06 | ms/batch 378.10 | loss  4.47 | ppl    87.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 139.23s | valid loss  4.50 | valid ppl    90.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 |   100/  331 batches | lr 7.81E-06 | ms/batch 377.14 | loss  4.56 | ppl    95.49\n",
      "| epoch  53 |   200/  331 batches | lr 7.81E-06 | ms/batch 376.28 | loss  4.57 | ppl    96.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = 1e20\n",
    "try:\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(valid_data, args.eval_batch_size)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "            epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        if val_loss < best_val_loss:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            lr *= LR_ANNEALING_RATE\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "with open(args.save, 'rb') as f:\n",
    "    model = torch.load(f, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on test data.\n",
    "test_loss = evaluate(test_data, args.test_batch_size)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
