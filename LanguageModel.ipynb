{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import WD_LSTM\n",
    "from data import Corpus\n",
    "from randomize_bptt import get_bptt_sequence_lengths\n",
    "from helpers import Config, repackage_hidden, batchify, get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "DATA = '/floyd/home/sein/'\n",
    "CUDA = True\n",
    "LOG_INTERVAL = 100\n",
    "LR_ANNEALING_RATE = 0.5\n",
    "CONFIG_NAME = 'language_model_base'\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
    "args = Config(CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "train_data = batchify(corpus.train, args.batch_size, device)\n",
    "valid_data = batchify(corpus.valid, args.eval_batch_size, device)\n",
    "test_data = batchify(corpus.test, args.test_batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WD_LSTM(\n",
       "  (variational_dropout): VariationalDropout()\n",
       "  (encoder): Embedding(18818, 400)\n",
       "  (rnns): ModuleList(\n",
       "    (0): WeightDrop(\n",
       "      (module): LSTM(400, 1150)\n",
       "    )\n",
       "    (1): WeightDrop(\n",
       "      (module): LSTM(1150, 1150)\n",
       "    )\n",
       "    (2): WeightDrop(\n",
       "      (module): LSTM(1150, 400)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=400, out_features=18818, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WD_LSTM(\n",
    "    ntoken=ntokens, \n",
    "    ninp=args.emsize,\n",
    "    nhid=args.nhid, \n",
    "    nlayers=args.nlayers, \n",
    "    dropout=args.dropout,\n",
    "    dropout_h=args.dropout_h,\n",
    "    dropout_i=args.dropout_i,\n",
    "    dropout_e=args.dropout_e,\n",
    "    weight_drop=args.weight_drop, \n",
    "    weight_tying=args.weight_tying\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = args.lr\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=args.weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source, batch_size):\n",
    "    model.eval()  # disable dropout\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, args.bptt_seq_len):\n",
    "            data, targets = get_batch(data_source, i, args.bptt_seq_len)\n",
    "            output, hidden, _, _ = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_tar_regularization_loss(outputs, dropped_outputs):\n",
    "    reg_loss = 0\n",
    "    for i in range(len(dropped_outputs[-1])):\n",
    "        reg_loss += args.alpha * dropped_outputs[-1][i].pow(2).mean()\n",
    "        if i >= 1:\n",
    "            reg_loss += args.beta * (outputs[-1][i] - outputs[-1][i - 1]).pow(2).mean()\n",
    "    return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args.batch_size)\n",
    "    for batch, (i, seq_len, lr_scale) in enumerate(get_bptt_sequence_lengths(\n",
    "        train_data.size(0), \n",
    "        args.bptt_seq_len, \n",
    "        args.bptt_random_scaling, \n",
    "        args.bptt_p, \n",
    "        args.bptt_s, \n",
    "        args.bptt_min_len\n",
    "    )):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr * lr_scale\n",
    "        data, targets = get_batch(train_data, i, seq_len)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden, outputs, dropped_outputs = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        unregularized_loss = criterion(output_flat, targets)\n",
    "        regularized_loss = unregularized_loss + ar_tar_regularization_loss(outputs, dropped_outputs)\n",
    "        regularized_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "        total_loss += unregularized_loss.item()\n",
    "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
    "            cur_loss = total_loss / LOG_INTERVAL\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:3.2E} | ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args.bptt_seq_len, lr,\n",
    "                elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py:477: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/  523 batches | lr 1.00E-03 | ms/batch 250.88 | loss  6.20 | ppl   494.79\n",
      "| epoch   1 |   200/  523 batches | lr 1.00E-03 | ms/batch 239.22 | loss  4.98 | ppl   145.60\n",
      "| epoch   1 |   300/  523 batches | lr 1.00E-03 | ms/batch 235.55 | loss  4.73 | ppl   113.71\n",
      "| epoch   1 |   400/  523 batches | lr 1.00E-03 | ms/batch 243.45 | loss  4.64 | ppl   103.29\n",
      "| epoch   1 |   500/  523 batches | lr 1.00E-03 | ms/batch 235.72 | loss  4.52 | ppl    92.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 143.37s | valid loss  4.45 | valid ppl    85.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   100/  523 batches | lr 1.00E-03 | ms/batch 239.76 | loss  4.39 | ppl    80.37\n",
      "| epoch   2 |   200/  523 batches | lr 1.00E-03 | ms/batch 234.91 | loss  4.26 | ppl    70.51\n",
      "| epoch   2 |   300/  523 batches | lr 1.00E-03 | ms/batch 243.75 | loss  4.23 | ppl    68.87\n",
      "| epoch   2 |   400/  523 batches | lr 1.00E-03 | ms/batch 238.27 | loss  4.24 | ppl    69.53\n",
      "| epoch   2 |   500/  523 batches | lr 1.00E-03 | ms/batch 237.52 | loss  4.22 | ppl    67.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 143.17s | valid loss  4.22 | valid ppl    68.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   100/  523 batches | lr 1.00E-03 | ms/batch 242.18 | loss  4.15 | ppl    63.34\n",
      "| epoch   3 |   200/  523 batches | lr 1.00E-03 | ms/batch 235.60 | loss  4.07 | ppl    58.40\n",
      "| epoch   3 |   300/  523 batches | lr 1.00E-03 | ms/batch 237.40 | loss  4.07 | ppl    58.28\n",
      "| epoch   3 |   400/  523 batches | lr 1.00E-03 | ms/batch 240.94 | loss  4.07 | ppl    58.56\n",
      "| epoch   3 |   500/  523 batches | lr 1.00E-03 | ms/batch 234.53 | loss  4.09 | ppl    59.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 143.30s | valid loss  4.11 | valid ppl    60.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   100/  523 batches | lr 1.00E-03 | ms/batch 243.59 | loss  4.03 | ppl    56.30\n",
      "| epoch   4 |   200/  523 batches | lr 1.00E-03 | ms/batch 242.70 | loss  3.95 | ppl    51.73\n",
      "| epoch   4 |   300/  523 batches | lr 1.00E-03 | ms/batch 242.88 | loss  3.94 | ppl    51.39\n",
      "| epoch   4 |   400/  523 batches | lr 1.00E-03 | ms/batch 239.72 | loss  3.99 | ppl    53.79\n",
      "| epoch   4 |   500/  523 batches | lr 1.00E-03 | ms/batch 240.43 | loss  3.97 | ppl    53.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 142.94s | valid loss  4.05 | valid ppl    57.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   100/  523 batches | lr 1.00E-03 | ms/batch 240.62 | loss  3.96 | ppl    52.44\n",
      "| epoch   5 |   200/  523 batches | lr 1.00E-03 | ms/batch 225.87 | loss  3.87 | ppl    47.85\n",
      "| epoch   5 |   300/  523 batches | lr 1.00E-03 | ms/batch 237.42 | loss  3.86 | ppl    47.62\n",
      "| epoch   5 |   400/  523 batches | lr 1.00E-03 | ms/batch 241.44 | loss  3.89 | ppl    49.05\n",
      "| epoch   5 |   500/  523 batches | lr 1.00E-03 | ms/batch 234.59 | loss  3.92 | ppl    50.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 143.49s | valid loss  4.00 | valid ppl    54.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   100/  523 batches | lr 1.00E-03 | ms/batch 239.54 | loss  3.87 | ppl    47.81\n",
      "| epoch   6 |   200/  523 batches | lr 1.00E-03 | ms/batch 245.66 | loss  3.80 | ppl    44.48\n",
      "| epoch   6 |   300/  523 batches | lr 1.00E-03 | ms/batch 235.84 | loss  3.80 | ppl    44.54\n",
      "| epoch   6 |   400/  523 batches | lr 1.00E-03 | ms/batch 242.11 | loss  3.85 | ppl    47.16\n",
      "| epoch   6 |   500/  523 batches | lr 1.00E-03 | ms/batch 234.46 | loss  3.84 | ppl    46.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 143.25s | valid loss  3.97 | valid ppl    52.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   100/  523 batches | lr 1.00E-03 | ms/batch 243.44 | loss  3.83 | ppl    45.88\n",
      "| epoch   7 |   200/  523 batches | lr 1.00E-03 | ms/batch 242.43 | loss  3.73 | ppl    41.63\n",
      "| epoch   7 |   300/  523 batches | lr 1.00E-03 | ms/batch 240.21 | loss  3.74 | ppl    41.92\n",
      "| epoch   7 |   400/  523 batches | lr 1.00E-03 | ms/batch 239.44 | loss  3.80 | ppl    44.51\n",
      "| epoch   7 |   500/  523 batches | lr 1.00E-03 | ms/batch 234.75 | loss  3.78 | ppl    43.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 143.28s | valid loss  3.94 | valid ppl    51.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   100/  523 batches | lr 1.00E-03 | ms/batch 242.61 | loss  3.76 | ppl    43.16\n",
      "| epoch   8 |   200/  523 batches | lr 1.00E-03 | ms/batch 245.40 | loss  3.70 | ppl    40.57\n",
      "| epoch   8 |   300/  523 batches | lr 1.00E-03 | ms/batch 249.26 | loss  3.70 | ppl    40.39\n",
      "| epoch   8 |   400/  523 batches | lr 1.00E-03 | ms/batch 237.17 | loss  3.75 | ppl    42.70\n",
      "| epoch   8 |   500/  523 batches | lr 1.00E-03 | ms/batch 237.10 | loss  3.74 | ppl    41.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 143.06s | valid loss  3.92 | valid ppl    50.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   100/  523 batches | lr 1.00E-03 | ms/batch 244.41 | loss  3.73 | ppl    41.69\n",
      "| epoch   9 |   200/  523 batches | lr 1.00E-03 | ms/batch 238.01 | loss  3.64 | ppl    38.10\n",
      "| epoch   9 |   300/  523 batches | lr 1.00E-03 | ms/batch 244.95 | loss  3.65 | ppl    38.50\n",
      "| epoch   9 |   400/  523 batches | lr 1.00E-03 | ms/batch 240.13 | loss  3.71 | ppl    40.82\n",
      "| epoch   9 |   500/  523 batches | lr 1.00E-03 | ms/batch 243.46 | loss  3.71 | ppl    40.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 143.12s | valid loss  3.90 | valid ppl    49.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   100/  523 batches | lr 1.00E-03 | ms/batch 239.31 | loss  3.69 | ppl    40.02\n",
      "| epoch  10 |   200/  523 batches | lr 1.00E-03 | ms/batch 240.76 | loss  3.62 | ppl    37.48\n",
      "| epoch  10 |   300/  523 batches | lr 1.00E-03 | ms/batch 231.24 | loss  3.62 | ppl    37.23\n",
      "| epoch  10 |   400/  523 batches | lr 1.00E-03 | ms/batch 236.41 | loss  3.66 | ppl    38.86\n",
      "| epoch  10 |   500/  523 batches | lr 1.00E-03 | ms/batch 237.63 | loss  3.70 | ppl    40.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 143.62s | valid loss  3.89 | valid ppl    48.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   100/  523 batches | lr 1.00E-03 | ms/batch 244.17 | loss  3.66 | ppl    38.73\n",
      "| epoch  11 |   200/  523 batches | lr 1.00E-03 | ms/batch 243.21 | loss  3.57 | ppl    35.62\n",
      "| epoch  11 |   300/  523 batches | lr 1.00E-03 | ms/batch 237.36 | loss  3.58 | ppl    35.77\n",
      "| epoch  11 |   400/  523 batches | lr 1.00E-03 | ms/batch 240.18 | loss  3.64 | ppl    37.91\n",
      "| epoch  11 |   500/  523 batches | lr 1.00E-03 | ms/batch 237.40 | loss  3.63 | ppl    37.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 143.22s | valid loss  3.88 | valid ppl    48.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   100/  523 batches | lr 1.00E-03 | ms/batch 244.92 | loss  3.60 | ppl    36.77\n",
      "| epoch  12 |   200/  523 batches | lr 1.00E-03 | ms/batch 234.48 | loss  3.55 | ppl    34.96\n",
      "| epoch  12 |   300/  523 batches | lr 1.00E-03 | ms/batch 241.67 | loss  3.54 | ppl    34.64\n",
      "| epoch  12 |   400/  523 batches | lr 1.00E-03 | ms/batch 239.46 | loss  3.60 | ppl    36.73\n",
      "| epoch  12 |   500/  523 batches | lr 1.00E-03 | ms/batch 240.15 | loss  3.60 | ppl    36.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 143.21s | valid loss  3.87 | valid ppl    48.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   100/  523 batches | lr 1.00E-03 | ms/batch 240.11 | loss  3.59 | ppl    36.07\n",
      "| epoch  13 |   200/  523 batches | lr 1.00E-03 | ms/batch 242.44 | loss  3.52 | ppl    33.66\n",
      "| epoch  13 |   300/  523 batches | lr 1.00E-03 | ms/batch 242.70 | loss  3.51 | ppl    33.61\n",
      "| epoch  13 |   400/  523 batches | lr 1.00E-03 | ms/batch 237.24 | loss  3.58 | ppl    35.90\n",
      "| epoch  13 |   500/  523 batches | lr 1.00E-03 | ms/batch 239.66 | loss  3.57 | ppl    35.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 143.42s | valid loss  3.87 | valid ppl    47.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   100/  523 batches | lr 1.00E-03 | ms/batch 240.12 | loss  3.56 | ppl    35.04\n",
      "| epoch  14 |   200/  523 batches | lr 1.00E-03 | ms/batch 238.01 | loss  3.49 | ppl    32.66\n",
      "| epoch  14 |   300/  523 batches | lr 1.00E-03 | ms/batch 244.19 | loss  3.50 | ppl    33.01\n",
      "| epoch  14 |   400/  523 batches | lr 1.00E-03 | ms/batch 243.25 | loss  3.55 | ppl    34.85\n",
      "| epoch  14 |   500/  523 batches | lr 1.00E-03 | ms/batch 241.41 | loss  3.56 | ppl    35.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 143.21s | valid loss  3.86 | valid ppl    47.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   100/  523 batches | lr 1.00E-03 | ms/batch 238.27 | loss  3.53 | ppl    34.27\n",
      "| epoch  15 |   200/  523 batches | lr 1.00E-03 | ms/batch 234.68 | loss  3.47 | ppl    32.04\n",
      "| epoch  15 |   300/  523 batches | lr 1.00E-03 | ms/batch 240.08 | loss  3.48 | ppl    32.45\n",
      "| epoch  15 |   400/  523 batches | lr 1.00E-03 | ms/batch 239.07 | loss  3.51 | ppl    33.56\n",
      "| epoch  15 |   500/  523 batches | lr 1.00E-03 | ms/batch 239.68 | loss  3.53 | ppl    34.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 143.49s | valid loss  3.85 | valid ppl    47.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   100/  523 batches | lr 1.00E-03 | ms/batch 236.61 | loss  3.51 | ppl    33.35\n",
      "| epoch  16 |   200/  523 batches | lr 1.00E-03 | ms/batch 237.40 | loss  3.43 | ppl    31.01\n",
      "| epoch  16 |   300/  523 batches | lr 1.00E-03 | ms/batch 236.04 | loss  3.44 | ppl    31.12\n",
      "| epoch  16 |   400/  523 batches | lr 1.00E-03 | ms/batch 240.46 | loss  3.50 | ppl    33.06\n",
      "| epoch  16 |   500/  523 batches | lr 1.00E-03 | ms/batch 237.87 | loss  3.51 | ppl    33.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 143.67s | valid loss  3.85 | valid ppl    46.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   100/  523 batches | lr 1.00E-03 | ms/batch 243.85 | loss  3.49 | ppl    32.81\n",
      "| epoch  17 |   200/  523 batches | lr 1.00E-03 | ms/batch 233.46 | loss  3.42 | ppl    30.49\n",
      "| epoch  17 |   300/  523 batches | lr 1.00E-03 | ms/batch 239.71 | loss  3.43 | ppl    30.99\n",
      "| epoch  17 |   400/  523 batches | lr 1.00E-03 | ms/batch 236.12 | loss  3.47 | ppl    32.29\n",
      "| epoch  17 |   500/  523 batches | lr 1.00E-03 | ms/batch 240.54 | loss  3.49 | ppl    32.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 143.63s | valid loss  3.85 | valid ppl    46.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   100/  523 batches | lr 5.00E-04 | ms/batch 240.83 | loss  3.46 | ppl    31.74\n",
      "| epoch  18 |   200/  523 batches | lr 5.00E-04 | ms/batch 248.60 | loss  3.38 | ppl    29.24\n",
      "| epoch  18 |   300/  523 batches | lr 5.00E-04 | ms/batch 238.54 | loss  3.37 | ppl    28.94\n",
      "| epoch  18 |   400/  523 batches | lr 5.00E-04 | ms/batch 241.35 | loss  3.42 | ppl    30.50\n",
      "| epoch  18 |   500/  523 batches | lr 5.00E-04 | ms/batch 242.03 | loss  3.42 | ppl    30.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 143.41s | valid loss  3.84 | valid ppl    46.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   100/  523 batches | lr 5.00E-04 | ms/batch 243.07 | loss  3.42 | ppl    30.68\n",
      "| epoch  19 |   200/  523 batches | lr 5.00E-04 | ms/batch 240.73 | loss  3.35 | ppl    28.39\n",
      "| epoch  19 |   300/  523 batches | lr 5.00E-04 | ms/batch 239.24 | loss  3.35 | ppl    28.58\n",
      "| epoch  19 |   400/  523 batches | lr 5.00E-04 | ms/batch 242.02 | loss  3.39 | ppl    29.70\n",
      "| epoch  19 |   500/  523 batches | lr 5.00E-04 | ms/batch 244.69 | loss  3.40 | ppl    29.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 143.61s | valid loss  3.85 | valid ppl    46.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   100/  523 batches | lr 2.50E-04 | ms/batch 239.07 | loss  3.40 | ppl    29.96\n",
      "| epoch  20 |   200/  523 batches | lr 2.50E-04 | ms/batch 239.54 | loss  3.31 | ppl    27.45\n",
      "| epoch  20 |   300/  523 batches | lr 2.50E-04 | ms/batch 242.87 | loss  3.33 | ppl    27.86\n",
      "| epoch  20 |   400/  523 batches | lr 2.50E-04 | ms/batch 242.67 | loss  3.38 | ppl    29.26\n",
      "| epoch  20 |   500/  523 batches | lr 2.50E-04 | ms/batch 237.67 | loss  3.37 | ppl    29.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 143.67s | valid loss  3.84 | valid ppl    46.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   100/  523 batches | lr 1.25E-04 | ms/batch 236.06 | loss  3.38 | ppl    29.27\n",
      "| epoch  21 |   200/  523 batches | lr 1.25E-04 | ms/batch 240.48 | loss  3.32 | ppl    27.69\n",
      "| epoch  21 |   300/  523 batches | lr 1.25E-04 | ms/batch 245.49 | loss  3.32 | ppl    27.56\n",
      "| epoch  21 |   400/  523 batches | lr 1.25E-04 | ms/batch 236.95 | loss  3.35 | ppl    28.48\n",
      "| epoch  21 |   500/  523 batches | lr 1.25E-04 | ms/batch 237.26 | loss  3.35 | ppl    28.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 143.72s | valid loss  3.85 | valid ppl    46.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   100/  523 batches | lr 6.25E-05 | ms/batch 239.62 | loss  3.39 | ppl    29.68\n",
      "| epoch  22 |   200/  523 batches | lr 6.25E-05 | ms/batch 233.42 | loss  3.31 | ppl    27.40\n",
      "| epoch  22 |   300/  523 batches | lr 6.25E-05 | ms/batch 240.87 | loss  3.30 | ppl    27.09\n",
      "| epoch  22 |   400/  523 batches | lr 6.25E-05 | ms/batch 245.52 | loss  3.33 | ppl    28.03\n",
      "| epoch  22 |   500/  523 batches | lr 6.25E-05 | ms/batch 235.97 | loss  3.33 | ppl    27.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 143.84s | valid loss  3.85 | valid ppl    46.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   100/  523 batches | lr 3.13E-05 | ms/batch 244.35 | loss  3.37 | ppl    29.06\n",
      "| epoch  23 |   200/  523 batches | lr 3.13E-05 | ms/batch 239.46 | loss  3.29 | ppl    26.91\n",
      "| epoch  23 |   300/  523 batches | lr 3.13E-05 | ms/batch 240.43 | loss  3.30 | ppl    27.04\n",
      "| epoch  23 |   400/  523 batches | lr 3.13E-05 | ms/batch 233.93 | loss  3.34 | ppl    28.26\n",
      "| epoch  23 |   500/  523 batches | lr 3.13E-05 | ms/batch 232.36 | loss  3.33 | ppl    28.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 143.73s | valid loss  3.85 | valid ppl    46.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   100/  523 batches | lr 1.56E-05 | ms/batch 248.11 | loss  3.38 | ppl    29.30\n",
      "| epoch  24 |   200/  523 batches | lr 1.56E-05 | ms/batch 251.40 | loss  3.31 | ppl    27.45\n",
      "| epoch  24 |   300/  523 batches | lr 1.56E-05 | ms/batch 238.13 | loss  3.30 | ppl    27.11\n",
      "| epoch  24 |   400/  523 batches | lr 1.56E-05 | ms/batch 241.94 | loss  3.33 | ppl    28.03\n",
      "| epoch  24 |   500/  523 batches | lr 1.56E-05 | ms/batch 242.46 | loss  3.33 | ppl    27.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 143.23s | valid loss  3.84 | valid ppl    46.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |   100/  523 batches | lr 7.81E-06 | ms/batch 236.52 | loss  3.38 | ppl    29.23\n",
      "| epoch  25 |   200/  523 batches | lr 7.81E-06 | ms/batch 233.98 | loss  3.29 | ppl    26.97\n",
      "| epoch  25 |   300/  523 batches | lr 7.81E-06 | ms/batch 242.08 | loss  3.29 | ppl    26.96\n",
      "| epoch  25 |   400/  523 batches | lr 7.81E-06 | ms/batch 239.19 | loss  3.33 | ppl    27.88\n",
      "| epoch  25 |   500/  523 batches | lr 7.81E-06 | ms/batch 240.52 | loss  3.35 | ppl    28.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 143.64s | valid loss  3.84 | valid ppl    46.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   100/  523 batches | lr 3.91E-06 | ms/batch 239.27 | loss  3.38 | ppl    29.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = 1e20\n",
    "try:\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(valid_data, args.eval_batch_size)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "            epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        if val_loss < best_val_loss:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            lr *= LR_ANNEALING_RATE\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "with open(args.save, 'rb') as f:\n",
    "    model = torch.load(f, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f95adc49f9fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run on test data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'| End of training | test loss {:5.2f} | test ppl {:8.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-85292d70c477>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(data_source, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepackage_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moutput_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss = evaluate(test_data, args.test_batch_size)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
