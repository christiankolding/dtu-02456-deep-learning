{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import WD_LSTM\n",
    "from data import Corpus\n",
    "from randomize_bptt import get_bptt_sequence_lengths\n",
    "from helpers import Config, repackage_hidden, batchify, get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "DATA = '/floyd/input/ptb/'\n",
    "CUDA = True\n",
    "LOG_INTERVAL = 50\n",
    "LR_ANNEALING_RATE = 0.25\n",
    "CONFIG_NAME = 'language_model_base'\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
    "args = Config(CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "train_data = batchify(corpus.train, args.batch_size, device)\n",
    "valid_data = batchify(corpus.valid, args.batch_size, device)\n",
    "test_data = batchify(corpus.test, args.batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WD_LSTM(\n",
       "  (drop): Dropout(p=0.2)\n",
       "  (encoder): Embedding(10000, 400)\n",
       "  (rnns): ModuleList(\n",
       "    (0): WeightDrop(\n",
       "      (module): LSTM(400, 800)\n",
       "    )\n",
       "    (1): WeightDrop(\n",
       "      (module): LSTM(800, 800)\n",
       "    )\n",
       "    (2): WeightDrop(\n",
       "      (module): LSTM(800, 400)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=800, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WD_LSTM(\n",
    "    ntokens, \n",
    "    args.emsize,\n",
    "    args.nhid, \n",
    "    args.nlayers, \n",
    "    args.dropout, \n",
    "    weight_drop=args.weight_drop, \n",
    "    weight_tying=args.weight_tying\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = args.lr\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    model.eval()  # disable dropout\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args.batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, args.bptt_seq_len):\n",
    "            data, targets = get_batch(data_source, i, args.bptt_seq_len)\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args.batch_size)\n",
    "    for batch, (i, seq_len, lr_scale) in enumerate(get_bptt_sequence_lengths(\n",
    "        train_data.size(0), \n",
    "        args.bptt_seq_len, \n",
    "        args.bptt_random_scaling, \n",
    "        args.bptt_p, \n",
    "        args.bptt_s, \n",
    "        args.bptt_min_len, \n",
    "        args.bptt_max_len\n",
    "    )):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr * lr_scale\n",
    "        data, targets = get_batch(train_data, i, seq_len)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
    "            cur_loss = total_loss / LOG_INTERVAL\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:3.2E} | ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args.bptt_seq_len, lr,\n",
    "                elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py:477: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    50/  663 batches | lr 2.00E-03 | ms/batch 166.92 | loss  7.05 | ppl  1155.82\n",
      "| epoch   1 |   100/  663 batches | lr 2.00E-03 | ms/batch 163.99 | loss  6.68 | ppl   796.50\n",
      "| epoch   1 |   150/  663 batches | lr 2.00E-03 | ms/batch 166.25 | loss  6.65 | ppl   774.77\n",
      "| epoch   1 |   200/  663 batches | lr 2.00E-03 | ms/batch 163.54 | loss  6.57 | ppl   715.65\n",
      "| epoch   1 |   250/  663 batches | lr 2.00E-03 | ms/batch 163.17 | loss  6.38 | ppl   590.44\n",
      "| epoch   1 |   300/  663 batches | lr 2.00E-03 | ms/batch 164.26 | loss  6.25 | ppl   518.19\n",
      "| epoch   1 |   350/  663 batches | lr 2.00E-03 | ms/batch 165.63 | loss  6.18 | ppl   483.32\n",
      "| epoch   1 |   400/  663 batches | lr 2.00E-03 | ms/batch 168.18 | loss  6.00 | ppl   404.92\n",
      "| epoch   1 |   450/  663 batches | lr 2.00E-03 | ms/batch 163.02 | loss  5.94 | ppl   379.36\n",
      "| epoch   1 |   500/  663 batches | lr 2.00E-03 | ms/batch 164.35 | loss  5.93 | ppl   376.73\n",
      "| epoch   1 |   550/  663 batches | lr 2.00E-03 | ms/batch 165.53 | loss  5.81 | ppl   334.43\n",
      "| epoch   1 |   600/  663 batches | lr 2.00E-03 | ms/batch 162.16 | loss  5.77 | ppl   321.62\n",
      "| epoch   1 |   650/  663 batches | lr 2.00E-03 | ms/batch 163.07 | loss  5.63 | ppl   277.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 116.11s | valid loss  5.62 | valid ppl   275.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    50/  663 batches | lr 2.00E-03 | ms/batch 170.29 | loss  5.70 | ppl   297.90\n",
      "| epoch   2 |   100/  663 batches | lr 2.00E-03 | ms/batch 164.11 | loss  5.62 | ppl   275.73\n",
      "| epoch   2 |   150/  663 batches | lr 2.00E-03 | ms/batch 159.86 | loss  5.59 | ppl   268.98\n",
      "| epoch   2 |   200/  663 batches | lr 2.00E-03 | ms/batch 161.62 | loss  5.53 | ppl   251.08\n",
      "| epoch   2 |   250/  663 batches | lr 2.00E-03 | ms/batch 166.65 | loss  5.51 | ppl   247.93\n",
      "| epoch   2 |   300/  663 batches | lr 2.00E-03 | ms/batch 156.13 | loss  5.49 | ppl   241.54\n",
      "| epoch   2 |   350/  663 batches | lr 2.00E-03 | ms/batch 164.28 | loss  5.50 | ppl   245.43\n",
      "| epoch   2 |   400/  663 batches | lr 2.00E-03 | ms/batch 165.78 | loss  5.34 | ppl   209.09\n",
      "| epoch   2 |   450/  663 batches | lr 2.00E-03 | ms/batch 165.21 | loss  5.35 | ppl   211.29\n",
      "| epoch   2 |   500/  663 batches | lr 2.00E-03 | ms/batch 163.05 | loss  5.37 | ppl   215.86\n",
      "| epoch   2 |   550/  663 batches | lr 2.00E-03 | ms/batch 166.10 | loss  5.30 | ppl   200.01\n",
      "| epoch   2 |   600/  663 batches | lr 2.00E-03 | ms/batch 167.21 | loss  5.26 | ppl   192.39\n",
      "| epoch   2 |   650/  663 batches | lr 2.00E-03 | ms/batch 165.16 | loss  5.15 | ppl   172.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 116.32s | valid loss  5.25 | valid ppl   189.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    50/  663 batches | lr 2.00E-03 | ms/batch 168.10 | loss  5.26 | ppl   193.17\n",
      "| epoch   3 |   100/  663 batches | lr 2.00E-03 | ms/batch 169.05 | loss  5.21 | ppl   183.97\n",
      "| epoch   3 |   150/  663 batches | lr 2.00E-03 | ms/batch 165.08 | loss  5.21 | ppl   183.40\n",
      "| epoch   3 |   200/  663 batches | lr 2.00E-03 | ms/batch 163.97 | loss  5.13 | ppl   169.60\n",
      "| epoch   3 |   250/  663 batches | lr 2.00E-03 | ms/batch 161.56 | loss  5.13 | ppl   168.52\n",
      "| epoch   3 |   300/  663 batches | lr 2.00E-03 | ms/batch 163.22 | loss  5.16 | ppl   174.10\n",
      "| epoch   3 |   350/  663 batches | lr 2.00E-03 | ms/batch 164.74 | loss  5.16 | ppl   174.49\n",
      "| epoch   3 |   400/  663 batches | lr 2.00E-03 | ms/batch 164.50 | loss  5.03 | ppl   152.65\n",
      "| epoch   3 |   450/  663 batches | lr 2.00E-03 | ms/batch 167.31 | loss  5.05 | ppl   155.73\n",
      "| epoch   3 |   500/  663 batches | lr 2.00E-03 | ms/batch 169.13 | loss  5.12 | ppl   166.64\n",
      "| epoch   3 |   550/  663 batches | lr 2.00E-03 | ms/batch 165.14 | loss  5.01 | ppl   149.27\n",
      "| epoch   3 |   600/  663 batches | lr 2.00E-03 | ms/batch 164.46 | loss  4.99 | ppl   147.04\n",
      "| epoch   3 |   650/  663 batches | lr 2.00E-03 | ms/batch 166.05 | loss  4.90 | ppl   134.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 116.16s | valid loss  5.07 | valid ppl   158.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    50/  663 batches | lr 2.00E-03 | ms/batch 166.37 | loss  5.02 | ppl   151.13\n",
      "| epoch   4 |   100/  663 batches | lr 2.00E-03 | ms/batch 167.86 | loss  4.98 | ppl   144.88\n",
      "| epoch   4 |   150/  663 batches | lr 2.00E-03 | ms/batch 169.85 | loss  4.99 | ppl   147.10\n",
      "| epoch   4 |   200/  663 batches | lr 2.00E-03 | ms/batch 160.51 | loss  4.89 | ppl   133.45\n",
      "| epoch   4 |   250/  663 batches | lr 2.00E-03 | ms/batch 167.40 | loss  4.90 | ppl   134.43\n",
      "| epoch   4 |   300/  663 batches | lr 2.00E-03 | ms/batch 160.67 | loss  4.94 | ppl   140.34\n",
      "| epoch   4 |   350/  663 batches | lr 2.00E-03 | ms/batch 163.74 | loss  4.94 | ppl   139.74\n",
      "| epoch   4 |   400/  663 batches | lr 2.00E-03 | ms/batch 163.13 | loss  4.82 | ppl   124.25\n",
      "| epoch   4 |   450/  663 batches | lr 2.00E-03 | ms/batch 167.24 | loss  4.85 | ppl   127.35\n",
      "| epoch   4 |   500/  663 batches | lr 2.00E-03 | ms/batch 170.62 | loss  4.94 | ppl   139.85\n",
      "| epoch   4 |   550/  663 batches | lr 2.00E-03 | ms/batch 167.12 | loss  4.82 | ppl   123.96\n",
      "| epoch   4 |   600/  663 batches | lr 2.00E-03 | ms/batch 164.03 | loss  4.81 | ppl   123.11\n",
      "| epoch   4 |   650/  663 batches | lr 2.00E-03 | ms/batch 160.97 | loss  4.71 | ppl   111.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 116.32s | valid loss  4.96 | valid ppl   142.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    50/  663 batches | lr 2.00E-03 | ms/batch 161.82 | loss  4.85 | ppl   127.14\n",
      "| epoch   5 |   100/  663 batches | lr 2.00E-03 | ms/batch 163.70 | loss  4.80 | ppl   121.82\n",
      "| epoch   5 |   150/  663 batches | lr 2.00E-03 | ms/batch 164.16 | loss  4.85 | ppl   127.38\n",
      "| epoch   5 |   200/  663 batches | lr 2.00E-03 | ms/batch 162.86 | loss  4.73 | ppl   113.43\n",
      "| epoch   5 |   250/  663 batches | lr 2.00E-03 | ms/batch 166.75 | loss  4.74 | ppl   114.57\n",
      "| epoch   5 |   300/  663 batches | lr 2.00E-03 | ms/batch 169.12 | loss  4.77 | ppl   118.42\n",
      "| epoch   5 |   350/  663 batches | lr 2.00E-03 | ms/batch 166.03 | loss  4.79 | ppl   120.88\n",
      "| epoch   5 |   400/  663 batches | lr 2.00E-03 | ms/batch 164.97 | loss  4.68 | ppl   107.55\n",
      "| epoch   5 |   450/  663 batches | lr 2.00E-03 | ms/batch 158.80 | loss  4.71 | ppl   111.46\n",
      "| epoch   5 |   500/  663 batches | lr 2.00E-03 | ms/batch 165.82 | loss  4.78 | ppl   118.58\n",
      "| epoch   5 |   550/  663 batches | lr 2.00E-03 | ms/batch 164.84 | loss  4.72 | ppl   112.40\n",
      "| epoch   5 |   600/  663 batches | lr 2.00E-03 | ms/batch 169.40 | loss  4.65 | ppl   104.69\n",
      "| epoch   5 |   650/  663 batches | lr 2.00E-03 | ms/batch 164.95 | loss  4.58 | ppl    97.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 116.39s | valid loss  4.89 | valid ppl   133.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    50/  663 batches | lr 2.00E-03 | ms/batch 165.86 | loss  4.71 | ppl   111.00\n",
      "| epoch   6 |   100/  663 batches | lr 2.00E-03 | ms/batch 160.18 | loss  4.68 | ppl   107.57\n",
      "| epoch   6 |   150/  663 batches | lr 2.00E-03 | ms/batch 158.91 | loss  4.72 | ppl   112.41\n",
      "| epoch   6 |   200/  663 batches | lr 2.00E-03 | ms/batch 164.64 | loss  4.60 | ppl    99.27\n",
      "| epoch   6 |   250/  663 batches | lr 2.00E-03 | ms/batch 167.84 | loss  4.60 | ppl    99.57\n",
      "| epoch   6 |   300/  663 batches | lr 2.00E-03 | ms/batch 164.69 | loss  4.64 | ppl   103.89\n",
      "| epoch   6 |   350/  663 batches | lr 2.00E-03 | ms/batch 163.91 | loss  4.68 | ppl   108.22\n",
      "| epoch   6 |   400/  663 batches | lr 2.00E-03 | ms/batch 167.14 | loss  4.55 | ppl    94.51\n",
      "| epoch   6 |   450/  663 batches | lr 2.00E-03 | ms/batch 164.63 | loss  4.60 | ppl    99.24\n",
      "| epoch   6 |   500/  663 batches | lr 2.00E-03 | ms/batch 164.99 | loss  4.66 | ppl   105.58\n",
      "| epoch   6 |   550/  663 batches | lr 2.00E-03 | ms/batch 162.81 | loss  4.61 | ppl   100.58\n",
      "| epoch   6 |   600/  663 batches | lr 2.00E-03 | ms/batch 162.61 | loss  4.52 | ppl    91.40\n",
      "| epoch   6 |   650/  663 batches | lr 2.00E-03 | ms/batch 165.13 | loss  4.48 | ppl    88.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 116.40s | valid loss  4.85 | valid ppl   127.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |    50/  663 batches | lr 2.00E-03 | ms/batch 162.67 | loss  4.60 | ppl    99.89\n",
      "| epoch   7 |   100/  663 batches | lr 2.00E-03 | ms/batch 167.38 | loss  4.57 | ppl    96.58\n",
      "| epoch   7 |   150/  663 batches | lr 2.00E-03 | ms/batch 164.58 | loss  4.62 | ppl   101.72\n",
      "| epoch   7 |   200/  663 batches | lr 2.00E-03 | ms/batch 164.65 | loss  4.48 | ppl    87.98\n",
      "| epoch   7 |   250/  663 batches | lr 2.00E-03 | ms/batch 170.42 | loss  4.49 | ppl    88.77\n",
      "| epoch   7 |   300/  663 batches | lr 2.00E-03 | ms/batch 158.62 | loss  4.54 | ppl    93.84\n",
      "| epoch   7 |   350/  663 batches | lr 2.00E-03 | ms/batch 168.10 | loss  4.57 | ppl    96.45\n",
      "| epoch   7 |   400/  663 batches | lr 2.00E-03 | ms/batch 165.27 | loss  4.44 | ppl    85.14\n",
      "| epoch   7 |   450/  663 batches | lr 2.00E-03 | ms/batch 160.03 | loss  4.49 | ppl    89.02\n",
      "| epoch   7 |   500/  663 batches | lr 2.00E-03 | ms/batch 162.09 | loss  4.56 | ppl    95.12\n",
      "| epoch   7 |   550/  663 batches | lr 2.00E-03 | ms/batch 162.80 | loss  4.51 | ppl    90.96\n",
      "| epoch   7 |   600/  663 batches | lr 2.00E-03 | ms/batch 164.01 | loss  4.42 | ppl    83.29\n",
      "| epoch   7 |   650/  663 batches | lr 2.00E-03 | ms/batch 167.21 | loss  4.38 | ppl    79.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 116.26s | valid loss  4.82 | valid ppl   123.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |    50/  663 batches | lr 2.00E-03 | ms/batch 169.04 | loss  4.50 | ppl    90.34\n",
      "| epoch   8 |   100/  663 batches | lr 2.00E-03 | ms/batch 164.47 | loss  4.49 | ppl    88.76\n",
      "| epoch   8 |   150/  663 batches | lr 2.00E-03 | ms/batch 161.32 | loss  4.53 | ppl    93.05\n",
      "| epoch   8 |   200/  663 batches | lr 2.00E-03 | ms/batch 166.49 | loss  4.38 | ppl    80.12\n",
      "| epoch   8 |   250/  663 batches | lr 2.00E-03 | ms/batch 165.05 | loss  4.40 | ppl    81.51\n",
      "| epoch   8 |   300/  663 batches | lr 2.00E-03 | ms/batch 164.53 | loss  4.46 | ppl    86.26\n",
      "| epoch   8 |   350/  663 batches | lr 2.00E-03 | ms/batch 164.92 | loss  4.47 | ppl    87.24\n",
      "| epoch   8 |   400/  663 batches | lr 2.00E-03 | ms/batch 166.35 | loss  4.35 | ppl    77.80\n",
      "| epoch   8 |   450/  663 batches | lr 2.00E-03 | ms/batch 165.07 | loss  4.40 | ppl    81.08\n",
      "| epoch   8 |   500/  663 batches | lr 2.00E-03 | ms/batch 165.48 | loss  4.50 | ppl    90.30\n",
      "| epoch   8 |   550/  663 batches | lr 2.00E-03 | ms/batch 159.28 | loss  4.41 | ppl    81.96\n",
      "| epoch   8 |   600/  663 batches | lr 2.00E-03 | ms/batch 168.39 | loss  4.34 | ppl    76.46\n",
      "| epoch   8 |   650/  663 batches | lr 2.00E-03 | ms/batch 166.52 | loss  4.30 | ppl    73.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 116.36s | valid loss  4.79 | valid ppl   119.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |    50/  663 batches | lr 2.00E-03 | ms/batch 171.44 | loss  4.41 | ppl    82.36\n",
      "| epoch   9 |   100/  663 batches | lr 2.00E-03 | ms/batch 166.54 | loss  4.41 | ppl    82.66\n",
      "| epoch   9 |   150/  663 batches | lr 2.00E-03 | ms/batch 162.32 | loss  4.46 | ppl    86.25\n",
      "| epoch   9 |   200/  663 batches | lr 2.00E-03 | ms/batch 164.12 | loss  4.30 | ppl    74.06\n",
      "| epoch   9 |   250/  663 batches | lr 2.00E-03 | ms/batch 167.95 | loss  4.32 | ppl    75.10\n",
      "| epoch   9 |   300/  663 batches | lr 2.00E-03 | ms/batch 167.30 | loss  4.38 | ppl    79.99\n",
      "| epoch   9 |   350/  663 batches | lr 2.00E-03 | ms/batch 161.68 | loss  4.39 | ppl    80.49\n",
      "| epoch   9 |   400/  663 batches | lr 2.00E-03 | ms/batch 161.66 | loss  4.28 | ppl    71.98\n",
      "| epoch   9 |   450/  663 batches | lr 2.00E-03 | ms/batch 164.51 | loss  4.33 | ppl    75.61\n",
      "| epoch   9 |   500/  663 batches | lr 2.00E-03 | ms/batch 165.94 | loss  4.43 | ppl    83.55\n",
      "| epoch   9 |   550/  663 batches | lr 2.00E-03 | ms/batch 162.90 | loss  4.32 | ppl    75.45\n",
      "| epoch   9 |   600/  663 batches | lr 2.00E-03 | ms/batch 166.61 | loss  4.27 | ppl    71.28\n",
      "| epoch   9 |   650/  663 batches | lr 2.00E-03 | ms/batch 166.02 | loss  4.22 | ppl    67.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 116.21s | valid loss  4.77 | valid ppl   117.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |    50/  663 batches | lr 2.00E-03 | ms/batch 167.77 | loss  4.33 | ppl    76.18\n",
      "| epoch  10 |   100/  663 batches | lr 2.00E-03 | ms/batch 163.89 | loss  4.35 | ppl    77.16\n",
      "| epoch  10 |   150/  663 batches | lr 2.00E-03 | ms/batch 161.83 | loss  4.39 | ppl    80.58\n",
      "| epoch  10 |   200/  663 batches | lr 2.00E-03 | ms/batch 164.33 | loss  4.24 | ppl    69.39\n",
      "| epoch  10 |   250/  663 batches | lr 2.00E-03 | ms/batch 167.97 | loss  4.25 | ppl    70.07\n",
      "| epoch  10 |   300/  663 batches | lr 2.00E-03 | ms/batch 163.14 | loss  4.31 | ppl    74.13\n",
      "| epoch  10 |   350/  663 batches | lr 2.00E-03 | ms/batch 165.31 | loss  4.32 | ppl    74.96\n",
      "| epoch  10 |   400/  663 batches | lr 2.00E-03 | ms/batch 156.48 | loss  4.22 | ppl    67.93\n",
      "| epoch  10 |   450/  663 batches | lr 2.00E-03 | ms/batch 166.38 | loss  4.27 | ppl    71.76\n",
      "| epoch  10 |   500/  663 batches | lr 2.00E-03 | ms/batch 166.75 | loss  4.32 | ppl    75.47\n",
      "| epoch  10 |   550/  663 batches | lr 2.00E-03 | ms/batch 165.39 | loss  4.28 | ppl    72.34\n",
      "| epoch  10 |   600/  663 batches | lr 2.00E-03 | ms/batch 163.78 | loss  4.19 | ppl    65.83\n",
      "| epoch  10 |   650/  663 batches | lr 2.00E-03 | ms/batch 165.90 | loss  4.16 | ppl    63.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 116.28s | valid loss  4.76 | valid ppl   116.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |    50/  663 batches | lr 2.00E-03 | ms/batch 173.38 | loss  4.27 | ppl    71.29\n",
      "| epoch  11 |   100/  663 batches | lr 2.00E-03 | ms/batch 169.18 | loss  4.30 | ppl    73.52\n",
      "| epoch  11 |   150/  663 batches | lr 2.00E-03 | ms/batch 164.60 | loss  4.31 | ppl    74.77\n",
      "| epoch  11 |   200/  663 batches | lr 2.00E-03 | ms/batch 161.62 | loss  4.19 | ppl    65.98\n",
      "| epoch  11 |   250/  663 batches | lr 2.00E-03 | ms/batch 169.29 | loss  4.19 | ppl    66.24\n",
      "| epoch  11 |   300/  663 batches | lr 2.00E-03 | ms/batch 155.24 | loss  4.23 | ppl    68.83\n",
      "| epoch  11 |   350/  663 batches | lr 2.00E-03 | ms/batch 166.40 | loss  4.26 | ppl    70.93\n",
      "| epoch  11 |   400/  663 batches | lr 2.00E-03 | ms/batch 168.30 | loss  4.15 | ppl    63.59\n",
      "| epoch  11 |   450/  663 batches | lr 2.00E-03 | ms/batch 162.80 | loss  4.20 | ppl    66.64\n",
      "| epoch  11 |   500/  663 batches | lr 2.00E-03 | ms/batch 163.48 | loss  4.31 | ppl    74.39\n",
      "| epoch  11 |   550/  663 batches | lr 2.00E-03 | ms/batch 160.85 | loss  4.21 | ppl    67.34\n",
      "| epoch  11 |   600/  663 batches | lr 2.00E-03 | ms/batch 164.35 | loss  4.13 | ppl    62.00\n",
      "| epoch  11 |   650/  663 batches | lr 2.00E-03 | ms/batch 162.94 | loss  4.09 | ppl    59.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 116.42s | valid loss  4.75 | valid ppl   115.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |    50/  663 batches | lr 2.00E-03 | ms/batch 169.50 | loss  4.21 | ppl    67.57\n",
      "| epoch  12 |   100/  663 batches | lr 2.00E-03 | ms/batch 164.67 | loss  4.22 | ppl    68.23\n",
      "| epoch  12 |   150/  663 batches | lr 2.00E-03 | ms/batch 162.39 | loss  4.28 | ppl    72.53\n",
      "| epoch  12 |   200/  663 batches | lr 2.00E-03 | ms/batch 162.53 | loss  4.13 | ppl    61.87\n",
      "| epoch  12 |   250/  663 batches | lr 2.00E-03 | ms/batch 168.05 | loss  4.13 | ppl    62.21\n",
      "| epoch  12 |   300/  663 batches | lr 2.00E-03 | ms/batch 165.61 | loss  4.19 | ppl    66.22\n",
      "| epoch  12 |   350/  663 batches | lr 2.00E-03 | ms/batch 170.81 | loss  4.21 | ppl    67.32\n",
      "| epoch  12 |   400/  663 batches | lr 2.00E-03 | ms/batch 161.74 | loss  4.11 | ppl    60.78\n",
      "| epoch  12 |   450/  663 batches | lr 2.00E-03 | ms/batch 166.41 | loss  4.16 | ppl    63.93\n",
      "| epoch  12 |   500/  663 batches | lr 2.00E-03 | ms/batch 156.75 | loss  4.24 | ppl    69.11\n",
      "| epoch  12 |   550/  663 batches | lr 2.00E-03 | ms/batch 168.01 | loss  4.16 | ppl    63.79\n",
      "| epoch  12 |   600/  663 batches | lr 2.00E-03 | ms/batch 164.61 | loss  4.08 | ppl    58.92\n",
      "| epoch  12 |   650/  663 batches | lr 2.00E-03 | ms/batch 165.62 | loss  4.03 | ppl    56.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 116.20s | valid loss  4.74 | valid ppl   114.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |    50/  663 batches | lr 2.00E-03 | ms/batch 164.45 | loss  4.15 | ppl    63.70\n",
      "| epoch  13 |   100/  663 batches | lr 2.00E-03 | ms/batch 167.54 | loss  4.17 | ppl    64.83\n",
      "| epoch  13 |   150/  663 batches | lr 2.00E-03 | ms/batch 157.99 | loss  4.23 | ppl    68.50\n",
      "| epoch  13 |   200/  663 batches | lr 2.00E-03 | ms/batch 163.11 | loss  4.07 | ppl    58.69\n",
      "| epoch  13 |   250/  663 batches | lr 2.00E-03 | ms/batch 162.19 | loss  4.10 | ppl    60.10\n",
      "| epoch  13 |   300/  663 batches | lr 2.00E-03 | ms/batch 161.20 | loss  4.12 | ppl    61.76\n",
      "| epoch  13 |   350/  663 batches | lr 2.00E-03 | ms/batch 163.52 | loss  4.17 | ppl    64.41\n",
      "| epoch  13 |   400/  663 batches | lr 2.00E-03 | ms/batch 162.67 | loss  4.05 | ppl    57.25\n",
      "| epoch  13 |   450/  663 batches | lr 2.00E-03 | ms/batch 163.15 | loss  4.11 | ppl    60.88\n",
      "| epoch  13 |   500/  663 batches | lr 2.00E-03 | ms/batch 166.71 | loss  4.17 | ppl    64.81\n",
      "| epoch  13 |   550/  663 batches | lr 2.00E-03 | ms/batch 161.57 | loss  4.14 | ppl    62.81\n",
      "| epoch  13 |   600/  663 batches | lr 2.00E-03 | ms/batch 158.76 | loss  4.03 | ppl    56.48\n",
      "| epoch  13 |   650/  663 batches | lr 2.00E-03 | ms/batch 163.63 | loss  3.98 | ppl    53.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 116.64s | valid loss  4.74 | valid ppl   114.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |    50/  663 batches | lr 5.00E-04 | ms/batch 171.99 | loss  4.14 | ppl    62.72\n",
      "| epoch  14 |   100/  663 batches | lr 5.00E-04 | ms/batch 166.60 | loss  4.15 | ppl    63.75\n",
      "| epoch  14 |   150/  663 batches | lr 5.00E-04 | ms/batch 167.61 | loss  4.21 | ppl    67.49\n",
      "| epoch  14 |   200/  663 batches | lr 5.00E-04 | ms/batch 162.12 | loss  4.07 | ppl    58.56\n",
      "| epoch  14 |   250/  663 batches | lr 5.00E-04 | ms/batch 167.79 | loss  4.05 | ppl    57.44\n",
      "| epoch  14 |   300/  663 batches | lr 5.00E-04 | ms/batch 162.80 | loss  4.11 | ppl    60.88\n",
      "| epoch  14 |   350/  663 batches | lr 5.00E-04 | ms/batch 164.00 | loss  4.10 | ppl    60.10\n",
      "| epoch  14 |   400/  663 batches | lr 5.00E-04 | ms/batch 162.61 | loss  3.97 | ppl    53.18\n",
      "| epoch  14 |   450/  663 batches | lr 5.00E-04 | ms/batch 167.17 | loss  4.01 | ppl    55.20\n",
      "| epoch  14 |   500/  663 batches | lr 5.00E-04 | ms/batch 162.17 | loss  4.11 | ppl    60.97\n",
      "| epoch  14 |   550/  663 batches | lr 5.00E-04 | ms/batch 165.37 | loss  3.99 | ppl    53.79\n",
      "| epoch  14 |   600/  663 batches | lr 5.00E-04 | ms/batch 163.49 | loss  3.89 | ppl    48.80\n",
      "| epoch  14 |   650/  663 batches | lr 5.00E-04 | ms/batch 162.22 | loss  3.86 | ppl    47.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 116.17s | valid loss  4.70 | valid ppl   109.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |    50/  663 batches | lr 5.00E-04 | ms/batch 166.18 | loss  4.08 | ppl    59.16\n",
      "| epoch  15 |   100/  663 batches | lr 5.00E-04 | ms/batch 169.32 | loss  4.10 | ppl    60.16\n",
      "| epoch  15 |   150/  663 batches | lr 5.00E-04 | ms/batch 167.90 | loss  4.16 | ppl    64.26\n",
      "| epoch  15 |   200/  663 batches | lr 5.00E-04 | ms/batch 163.70 | loss  4.03 | ppl    56.19\n",
      "| epoch  15 |   250/  663 batches | lr 5.00E-04 | ms/batch 167.16 | loss  4.02 | ppl    55.43\n",
      "| epoch  15 |   300/  663 batches | lr 5.00E-04 | ms/batch 162.65 | loss  4.07 | ppl    58.70\n",
      "| epoch  15 |   350/  663 batches | lr 5.00E-04 | ms/batch 167.83 | loss  4.08 | ppl    58.92\n",
      "| epoch  15 |   400/  663 batches | lr 5.00E-04 | ms/batch 164.95 | loss  3.93 | ppl    51.01\n",
      "| epoch  15 |   450/  663 batches | lr 5.00E-04 | ms/batch 160.04 | loss  4.00 | ppl    54.52\n",
      "| epoch  15 |   500/  663 batches | lr 5.00E-04 | ms/batch 169.20 | loss  4.08 | ppl    59.28\n",
      "| epoch  15 |   550/  663 batches | lr 5.00E-04 | ms/batch 161.15 | loss  3.96 | ppl    52.23\n",
      "| epoch  15 |   600/  663 batches | lr 5.00E-04 | ms/batch 163.98 | loss  3.89 | ppl    48.72\n",
      "| epoch  15 |   650/  663 batches | lr 5.00E-04 | ms/batch 159.40 | loss  3.84 | ppl    46.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 116.22s | valid loss  4.69 | valid ppl   108.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |    50/  663 batches | lr 5.00E-04 | ms/batch 168.35 | loss  4.05 | ppl    57.58\n",
      "| epoch  16 |   100/  663 batches | lr 5.00E-04 | ms/batch 160.02 | loss  4.06 | ppl    57.91\n",
      "| epoch  16 |   150/  663 batches | lr 5.00E-04 | ms/batch 167.98 | loss  4.15 | ppl    63.33\n",
      "| epoch  16 |   200/  663 batches | lr 5.00E-04 | ms/batch 162.20 | loss  3.98 | ppl    53.42\n",
      "| epoch  16 |   250/  663 batches | lr 5.00E-04 | ms/batch 159.33 | loss  3.99 | ppl    54.24\n",
      "| epoch  16 |   300/  663 batches | lr 5.00E-04 | ms/batch 168.92 | loss  4.03 | ppl    56.09\n",
      "| epoch  16 |   350/  663 batches | lr 5.00E-04 | ms/batch 169.80 | loss  4.05 | ppl    57.27\n",
      "| epoch  16 |   400/  663 batches | lr 5.00E-04 | ms/batch 168.72 | loss  3.92 | ppl    50.39\n",
      "| epoch  16 |   450/  663 batches | lr 5.00E-04 | ms/batch 168.71 | loss  3.96 | ppl    52.69\n",
      "| epoch  16 |   500/  663 batches | lr 5.00E-04 | ms/batch 165.71 | loss  4.06 | ppl    57.73\n",
      "| epoch  16 |   550/  663 batches | lr 5.00E-04 | ms/batch 167.42 | loss  3.93 | ppl    50.94\n",
      "| epoch  16 |   600/  663 batches | lr 5.00E-04 | ms/batch 168.08 | loss  3.86 | ppl    47.70\n",
      "| epoch  16 |   650/  663 batches | lr 5.00E-04 | ms/batch 163.09 | loss  3.83 | ppl    46.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 116.44s | valid loss  4.68 | valid ppl   108.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |    50/  663 batches | lr 5.00E-04 | ms/batch 170.91 | loss  4.02 | ppl    55.74\n",
      "| epoch  17 |   100/  663 batches | lr 5.00E-04 | ms/batch 164.71 | loss  4.04 | ppl    57.06\n",
      "| epoch  17 |   150/  663 batches | lr 5.00E-04 | ms/batch 168.33 | loss  4.10 | ppl    60.39\n",
      "| epoch  17 |   200/  663 batches | lr 5.00E-04 | ms/batch 162.08 | loss  3.98 | ppl    53.25\n",
      "| epoch  17 |   250/  663 batches | lr 5.00E-04 | ms/batch 159.38 | loss  3.95 | ppl    51.84\n",
      "| epoch  17 |   300/  663 batches | lr 5.00E-04 | ms/batch 164.13 | loss  4.02 | ppl    55.61\n",
      "| epoch  17 |   350/  663 batches | lr 5.00E-04 | ms/batch 166.62 | loss  4.02 | ppl    55.83\n",
      "| epoch  17 |   400/  663 batches | lr 5.00E-04 | ms/batch 164.08 | loss  3.91 | ppl    49.68\n",
      "| epoch  17 |   450/  663 batches | lr 5.00E-04 | ms/batch 168.02 | loss  3.94 | ppl    51.60\n",
      "| epoch  17 |   500/  663 batches | lr 5.00E-04 | ms/batch 161.66 | loss  4.04 | ppl    56.76\n",
      "| epoch  17 |   550/  663 batches | lr 5.00E-04 | ms/batch 167.02 | loss  3.94 | ppl    51.57\n",
      "| epoch  17 |   600/  663 batches | lr 5.00E-04 | ms/batch 166.46 | loss  3.83 | ppl    46.23\n",
      "| epoch  17 |   650/  663 batches | lr 5.00E-04 | ms/batch 167.00 | loss  3.82 | ppl    45.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 116.43s | valid loss  4.68 | valid ppl   107.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |    50/  663 batches | lr 5.00E-04 | ms/batch 172.17 | loss  4.00 | ppl    54.38\n",
      "| epoch  18 |   100/  663 batches | lr 5.00E-04 | ms/batch 163.02 | loss  4.02 | ppl    55.69\n",
      "| epoch  18 |   150/  663 batches | lr 5.00E-04 | ms/batch 162.65 | loss  4.09 | ppl    59.67\n",
      "| epoch  18 |   200/  663 batches | lr 5.00E-04 | ms/batch 162.15 | loss  3.93 | ppl    50.78\n",
      "| epoch  18 |   250/  663 batches | lr 5.00E-04 | ms/batch 164.11 | loss  3.93 | ppl    50.96\n",
      "| epoch  18 |   300/  663 batches | lr 5.00E-04 | ms/batch 167.82 | loss  3.98 | ppl    53.77\n",
      "| epoch  18 |   350/  663 batches | lr 5.00E-04 | ms/batch 169.36 | loss  4.01 | ppl    54.94\n",
      "| epoch  18 |   400/  663 batches | lr 5.00E-04 | ms/batch 162.63 | loss  3.88 | ppl    48.26\n",
      "| epoch  18 |   450/  663 batches | lr 5.00E-04 | ms/batch 169.75 | loss  3.92 | ppl    50.54\n",
      "| epoch  18 |   500/  663 batches | lr 5.00E-04 | ms/batch 167.60 | loss  4.02 | ppl    55.81\n",
      "| epoch  18 |   550/  663 batches | lr 5.00E-04 | ms/batch 161.85 | loss  3.91 | ppl    49.87\n",
      "| epoch  18 |   600/  663 batches | lr 5.00E-04 | ms/batch 162.34 | loss  3.82 | ppl    45.51\n",
      "| epoch  18 |   650/  663 batches | lr 5.00E-04 | ms/batch 168.05 | loss  3.79 | ppl    44.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 116.63s | valid loss  4.68 | valid ppl   107.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |    50/  663 batches | lr 1.25E-04 | ms/batch 167.59 | loss  3.99 | ppl    54.11\n",
      "| epoch  19 |   100/  663 batches | lr 1.25E-04 | ms/batch 168.04 | loss  4.05 | ppl    57.56\n",
      "| epoch  19 |   150/  663 batches | lr 1.25E-04 | ms/batch 161.97 | loss  4.12 | ppl    61.61\n",
      "| epoch  19 |   200/  663 batches | lr 1.25E-04 | ms/batch 164.95 | loss  3.93 | ppl    51.01\n",
      "| epoch  19 |   250/  663 batches | lr 1.25E-04 | ms/batch 167.75 | loss  3.95 | ppl    51.95\n",
      "| epoch  19 |   300/  663 batches | lr 1.25E-04 | ms/batch 165.30 | loss  3.99 | ppl    54.21\n",
      "| epoch  19 |   350/  663 batches | lr 1.25E-04 | ms/batch 164.03 | loss  3.99 | ppl    54.06\n",
      "| epoch  19 |   400/  663 batches | lr 1.25E-04 | ms/batch 165.51 | loss  3.88 | ppl    48.34\n",
      "| epoch  19 |   450/  663 batches | lr 1.25E-04 | ms/batch 163.31 | loss  3.92 | ppl    50.19\n",
      "| epoch  19 |   500/  663 batches | lr 1.25E-04 | ms/batch 168.08 | loss  4.02 | ppl    55.56\n",
      "| epoch  19 |   550/  663 batches | lr 1.25E-04 | ms/batch 166.66 | loss  3.88 | ppl    48.20\n",
      "| epoch  19 |   600/  663 batches | lr 1.25E-04 | ms/batch 161.45 | loss  3.83 | ppl    45.93\n",
      "| epoch  19 |   650/  663 batches | lr 1.25E-04 | ms/batch 166.67 | loss  3.76 | ppl    42.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 116.40s | valid loss  4.65 | valid ppl   104.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |    50/  663 batches | lr 1.25E-04 | ms/batch 169.21 | loss  4.00 | ppl    54.57\n",
      "| epoch  20 |   100/  663 batches | lr 1.25E-04 | ms/batch 162.73 | loss  4.02 | ppl    55.73\n",
      "| epoch  20 |   150/  663 batches | lr 1.25E-04 | ms/batch 165.18 | loss  4.09 | ppl    59.95\n",
      "| epoch  20 |   200/  663 batches | lr 1.25E-04 | ms/batch 163.27 | loss  3.91 | ppl    49.94\n",
      "| epoch  20 |   250/  663 batches | lr 1.25E-04 | ms/batch 167.68 | loss  3.94 | ppl    51.35\n",
      "| epoch  20 |   300/  663 batches | lr 1.25E-04 | ms/batch 165.18 | loss  3.98 | ppl    53.51\n",
      "| epoch  20 |   350/  663 batches | lr 1.25E-04 | ms/batch 163.10 | loss  3.97 | ppl    52.88\n",
      "| epoch  20 |   400/  663 batches | lr 1.25E-04 | ms/batch 168.08 | loss  3.87 | ppl    48.09\n",
      "| epoch  20 |   450/  663 batches | lr 1.25E-04 | ms/batch 164.46 | loss  3.91 | ppl    50.10\n",
      "| epoch  20 |   500/  663 batches | lr 1.25E-04 | ms/batch 167.77 | loss  4.01 | ppl    55.03\n",
      "| epoch  20 |   550/  663 batches | lr 1.25E-04 | ms/batch 161.63 | loss  3.89 | ppl    48.67\n",
      "| epoch  20 |   600/  663 batches | lr 1.25E-04 | ms/batch 162.05 | loss  3.79 | ppl    44.06\n",
      "| epoch  20 |   650/  663 batches | lr 1.25E-04 | ms/batch 165.43 | loss  3.76 | ppl    42.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 116.46s | valid loss  4.65 | valid ppl   104.77\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = 1e20\n",
    "try:\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(valid_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "            epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        if val_loss < best_val_loss:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            lr *= LR_ANNEALING_RATE\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "with open(args.save, 'rb') as f:\n",
    "    model = torch.load(f, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  4.58 | test ppl    97.39\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
