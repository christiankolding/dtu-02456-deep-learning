{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import WD_LSTM\n",
    "from data import Corpus\n",
    "from randomize_bptt import get_bptt_sequence_lengths\n",
    "from helpers import Config, repackage_hidden, batchify, get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "DATA = '/floyd/home/wt2/'  # '/floyd/home/ptb/'\n",
    "CUDA = True\n",
    "LOG_INTERVAL = 200\n",
    "LR_ANNEALING_RATE = 0.5\n",
    "CONFIG_NAME = 'language_model_wt2'  # 'language_model_base'\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
    "args = Config(CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "train_data = batchify(corpus.train, args.batch_size, device)\n",
    "valid_data = batchify(corpus.valid, args.eval_batch_size, device)\n",
    "test_data = batchify(corpus.test, args.test_batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WD_LSTM(\n",
       "  (variational_dropout): VariationalDropout()\n",
       "  (encoder): Embedding(33278, 400)\n",
       "  (rnns): ModuleList(\n",
       "    (0): WeightDrop(\n",
       "      (module): LSTM(400, 1150)\n",
       "    )\n",
       "    (1): WeightDrop(\n",
       "      (module): LSTM(1150, 1150)\n",
       "    )\n",
       "    (2): WeightDrop(\n",
       "      (module): LSTM(1150, 400)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=400, out_features=33278, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WD_LSTM(\n",
    "    ntoken=ntokens, \n",
    "    ninp=args.emsize,\n",
    "    nhid=args.nhid, \n",
    "    nlayers=args.nlayers, \n",
    "    dropout=args.dropout,\n",
    "    dropout_h=args.dropout_h,\n",
    "    dropout_i=args.dropout_i,\n",
    "    dropout_e=args.dropout_e,\n",
    "    weight_drop=args.weight_drop, \n",
    "    weight_tying=args.weight_tying\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = args.lr\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=args.weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source, batch_size):\n",
    "    model.eval()  # disable dropout\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, args.bptt_seq_len):\n",
    "            data, targets = get_batch(data_source, i, args.bptt_seq_len)\n",
    "            output, hidden, _, _ = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_tar_regularization_loss(outputs, dropped_outputs):\n",
    "    reg_loss = 0\n",
    "    for i in range(len(dropped_outputs[-1])):\n",
    "        reg_loss += args.alpha * dropped_outputs[-1][i].pow(2).mean()\n",
    "        if i >= 1:\n",
    "            reg_loss += args.beta * (outputs[-1][i] - outputs[-1][i - 1]).pow(2).mean()\n",
    "    return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args.batch_size)\n",
    "    for batch, (i, seq_len, lr_scale) in enumerate(get_bptt_sequence_lengths(\n",
    "        train_data.size(0), \n",
    "        args.bptt_seq_len, \n",
    "        args.bptt_random_scaling, \n",
    "        args.bptt_p, \n",
    "        args.bptt_s, \n",
    "        args.bptt_min_len\n",
    "    )):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr * lr_scale\n",
    "        data, targets = get_batch(train_data, i, seq_len)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden, outputs, dropped_outputs = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        unregularized_loss = criterion(output_flat, targets)\n",
    "        regularized_loss = unregularized_loss + ar_tar_regularization_loss(outputs, dropped_outputs)\n",
    "        regularized_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "        total_loss += unregularized_loss.item()\n",
    "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
    "            cur_loss = total_loss / LOG_INTERVAL\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:3.2E} | ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args.bptt_seq_len, lr,\n",
    "                elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py:477: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/  745 batches | lr 1.00E-03 | ms/batch 470.48 | loss  7.31 | ppl  1491.36\n",
      "| epoch   1 |   400/  745 batches | lr 1.00E-03 | ms/batch 461.35 | loss  6.67 | ppl   791.08\n",
      "| epoch   1 |   600/  745 batches | lr 1.00E-03 | ms/batch 456.92 | loss  6.44 | ppl   627.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 391.69s | valid loss  5.90 | valid ppl   364.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/  745 batches | lr 1.00E-03 | ms/batch 470.31 | loss  6.16 | ppl   473.85\n",
      "| epoch   2 |   400/  745 batches | lr 1.00E-03 | ms/batch 450.27 | loss  6.04 | ppl   418.23\n",
      "| epoch   2 |   600/  745 batches | lr 1.00E-03 | ms/batch 458.43 | loss  5.91 | ppl   369.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 392.57s | valid loss  5.50 | valid ppl   245.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/  745 batches | lr 1.00E-03 | ms/batch 459.02 | loss  5.81 | ppl   333.89\n",
      "| epoch   3 |   400/  745 batches | lr 1.00E-03 | ms/batch 467.55 | loss  5.71 | ppl   302.70\n",
      "| epoch   3 |   600/  745 batches | lr 1.00E-03 | ms/batch 464.07 | loss  5.62 | ppl   276.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 390.60s | valid loss  5.27 | valid ppl   193.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/  745 batches | lr 1.00E-03 | ms/batch 450.37 | loss  5.57 | ppl   263.06\n",
      "| epoch   4 |   400/  745 batches | lr 1.00E-03 | ms/batch 453.91 | loss  5.50 | ppl   245.31\n",
      "| epoch   4 |   600/  745 batches | lr 1.00E-03 | ms/batch 465.47 | loss  5.43 | ppl   227.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 389.88s | valid loss  5.12 | valid ppl   167.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/  745 batches | lr 1.00E-03 | ms/batch 457.78 | loss  5.41 | ppl   223.06\n",
      "| epoch   5 |   400/  745 batches | lr 1.00E-03 | ms/batch 465.12 | loss  5.36 | ppl   212.54\n",
      "| epoch   5 |   600/  745 batches | lr 1.00E-03 | ms/batch 455.66 | loss  5.28 | ppl   196.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 390.81s | valid loss  5.02 | valid ppl   151.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/  745 batches | lr 1.00E-03 | ms/batch 476.63 | loss  5.29 | ppl   198.06\n",
      "| epoch   6 |   400/  745 batches | lr 1.00E-03 | ms/batch 459.21 | loss  5.23 | ppl   186.57\n",
      "| epoch   6 |   600/  745 batches | lr 1.00E-03 | ms/batch 462.58 | loss  5.16 | ppl   174.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 390.10s | valid loss  4.93 | valid ppl   139.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/  745 batches | lr 1.00E-03 | ms/batch 466.30 | loss  5.19 | ppl   180.18\n",
      "| epoch   7 |   400/  745 batches | lr 1.00E-03 | ms/batch 454.29 | loss  5.14 | ppl   170.50\n",
      "| epoch   7 |   600/  745 batches | lr 1.00E-03 | ms/batch 450.84 | loss  5.07 | ppl   159.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 389.11s | valid loss  4.87 | valid ppl   130.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/  745 batches | lr 1.00E-03 | ms/batch 462.70 | loss  5.10 | ppl   163.43\n",
      "| epoch   8 |   400/  745 batches | lr 1.00E-03 | ms/batch 456.21 | loss  5.05 | ppl   155.64\n",
      "| epoch   8 |   600/  745 batches | lr 1.00E-03 | ms/batch 459.36 | loss  5.01 | ppl   149.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 389.04s | valid loss  4.82 | valid ppl   123.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/  745 batches | lr 1.00E-03 | ms/batch 468.20 | loss  5.03 | ppl   153.31\n",
      "| epoch   9 |   400/  745 batches | lr 1.00E-03 | ms/batch 463.66 | loss  4.99 | ppl   146.95\n",
      "| epoch   9 |   600/  745 batches | lr 1.00E-03 | ms/batch 460.11 | loss  4.93 | ppl   138.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 390.78s | valid loss  4.77 | valid ppl   118.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/  745 batches | lr 1.00E-03 | ms/batch 451.76 | loss  4.96 | ppl   142.94\n",
      "| epoch  10 |   400/  745 batches | lr 1.00E-03 | ms/batch 463.02 | loss  4.93 | ppl   138.63\n",
      "| epoch  10 |   600/  745 batches | lr 1.00E-03 | ms/batch 467.69 | loss  4.87 | ppl   130.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 390.65s | valid loss  4.73 | valid ppl   113.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/  745 batches | lr 1.00E-03 | ms/batch 459.85 | loss  4.91 | ppl   135.80\n",
      "| epoch  11 |   400/  745 batches | lr 1.00E-03 | ms/batch 460.31 | loss  4.88 | ppl   131.96\n",
      "| epoch  11 |   600/  745 batches | lr 1.00E-03 | ms/batch 451.55 | loss  4.83 | ppl   125.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 390.81s | valid loss  4.70 | valid ppl   109.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/  745 batches | lr 1.00E-03 | ms/batch 461.87 | loss  4.86 | ppl   128.82\n",
      "| epoch  12 |   400/  745 batches | lr 1.00E-03 | ms/batch 454.64 | loss  4.83 | ppl   125.58\n",
      "| epoch  12 |   600/  745 batches | lr 1.00E-03 | ms/batch 455.34 | loss  4.79 | ppl   120.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 391.82s | valid loss  4.67 | valid ppl   106.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/  745 batches | lr 1.00E-03 | ms/batch 471.66 | loss  4.82 | ppl   123.54\n",
      "| epoch  13 |   400/  745 batches | lr 1.00E-03 | ms/batch 452.61 | loss  4.79 | ppl   120.46\n",
      "| epoch  13 |   600/  745 batches | lr 1.00E-03 | ms/batch 459.51 | loss  4.74 | ppl   114.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 389.30s | valid loss  4.65 | valid ppl   104.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/  745 batches | lr 1.00E-03 | ms/batch 462.61 | loss  4.78 | ppl   119.40\n",
      "| epoch  14 |   400/  745 batches | lr 1.00E-03 | ms/batch 456.94 | loss  4.75 | ppl   115.92\n",
      "| epoch  14 |   600/  745 batches | lr 1.00E-03 | ms/batch 464.74 | loss  4.71 | ppl   111.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 390.72s | valid loss  4.63 | valid ppl   102.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/  745 batches | lr 1.00E-03 | ms/batch 453.66 | loss  4.74 | ppl   114.99\n",
      "| epoch  15 |   400/  745 batches | lr 1.00E-03 | ms/batch 471.00 | loss  4.72 | ppl   111.99\n",
      "| epoch  15 |   600/  745 batches | lr 1.00E-03 | ms/batch 453.08 | loss  4.68 | ppl   107.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 390.54s | valid loss  4.61 | valid ppl    99.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/  745 batches | lr 1.00E-03 | ms/batch 471.45 | loss  4.71 | ppl   110.68\n",
      "| epoch  16 |   400/  745 batches | lr 1.00E-03 | ms/batch 458.98 | loss  4.68 | ppl   107.98\n",
      "| epoch  16 |   600/  745 batches | lr 1.00E-03 | ms/batch 457.60 | loss  4.64 | ppl   103.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 390.66s | valid loss  4.59 | valid ppl    98.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/  745 batches | lr 1.00E-03 | ms/batch 458.40 | loss  4.68 | ppl   107.42\n",
      "| epoch  17 |   400/  745 batches | lr 1.00E-03 | ms/batch 468.61 | loss  4.65 | ppl   104.89\n",
      "| epoch  17 |   600/  745 batches | lr 1.00E-03 | ms/batch 462.68 | loss  4.60 | ppl    99.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 389.74s | valid loss  4.57 | valid ppl    96.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/  745 batches | lr 1.00E-03 | ms/batch 456.44 | loss  4.65 | ppl   104.99\n",
      "| epoch  18 |   400/  745 batches | lr 1.00E-03 | ms/batch 462.04 | loss  4.63 | ppl   102.22\n",
      "| epoch  18 |   600/  745 batches | lr 1.00E-03 | ms/batch 454.03 | loss  4.58 | ppl    97.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 389.53s | valid loss  4.56 | valid ppl    95.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/  745 batches | lr 1.00E-03 | ms/batch 463.86 | loss  4.63 | ppl   102.41\n",
      "| epoch  19 |   400/  745 batches | lr 1.00E-03 | ms/batch 458.64 | loss  4.61 | ppl   100.00\n",
      "| epoch  19 |   600/  745 batches | lr 1.00E-03 | ms/batch 464.30 | loss  4.55 | ppl    94.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 389.47s | valid loss  4.54 | valid ppl    94.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/  745 batches | lr 1.00E-03 | ms/batch 471.11 | loss  4.60 | ppl    99.93\n",
      "| epoch  20 |   400/  745 batches | lr 1.00E-03 | ms/batch 466.76 | loss  4.57 | ppl    96.94\n",
      "| epoch  20 |   600/  745 batches | lr 1.00E-03 | ms/batch 464.48 | loss  4.53 | ppl    93.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 390.42s | valid loss  4.53 | valid ppl    93.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   200/  745 batches | lr 1.00E-03 | ms/batch 467.94 | loss  4.59 | ppl    98.03\n",
      "| epoch  21 |   400/  745 batches | lr 1.00E-03 | ms/batch 459.99 | loss  4.56 | ppl    95.90\n",
      "| epoch  21 |   600/  745 batches | lr 1.00E-03 | ms/batch 459.60 | loss  4.51 | ppl    90.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 390.23s | valid loss  4.52 | valid ppl    92.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   200/  745 batches | lr 1.00E-03 | ms/batch 467.06 | loss  4.56 | ppl    95.57\n",
      "| epoch  22 |   400/  745 batches | lr 1.00E-03 | ms/batch 453.89 | loss  4.54 | ppl    93.73\n",
      "| epoch  22 |   600/  745 batches | lr 1.00E-03 | ms/batch 457.44 | loss  4.49 | ppl    89.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 389.93s | valid loss  4.52 | valid ppl    91.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   200/  745 batches | lr 1.00E-03 | ms/batch 458.12 | loss  4.54 | ppl    93.77\n",
      "| epoch  23 |   400/  745 batches | lr 1.00E-03 | ms/batch 461.23 | loss  4.53 | ppl    92.32\n",
      "| epoch  23 |   600/  745 batches | lr 1.00E-03 | ms/batch 455.71 | loss  4.48 | ppl    88.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 391.28s | valid loss  4.50 | valid ppl    90.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   200/  745 batches | lr 1.00E-03 | ms/batch 460.61 | loss  4.52 | ppl    91.89\n",
      "| epoch  24 |   400/  745 batches | lr 1.00E-03 | ms/batch 459.94 | loss  4.51 | ppl    91.14\n",
      "| epoch  24 |   600/  745 batches | lr 1.00E-03 | ms/batch 463.14 | loss  4.45 | ppl    85.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 390.59s | valid loss  4.49 | valid ppl    89.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |   200/  745 batches | lr 1.00E-03 | ms/batch 460.27 | loss  4.50 | ppl    90.39\n",
      "| epoch  25 |   400/  745 batches | lr 1.00E-03 | ms/batch 455.58 | loss  4.49 | ppl    89.24\n",
      "| epoch  25 |   600/  745 batches | lr 1.00E-03 | ms/batch 461.52 | loss  4.45 | ppl    85.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 390.34s | valid loss  4.49 | valid ppl    88.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   200/  745 batches | lr 1.00E-03 | ms/batch 458.08 | loss  4.49 | ppl    89.17\n",
      "| epoch  26 |   400/  745 batches | lr 1.00E-03 | ms/batch 453.25 | loss  4.46 | ppl    86.91\n",
      "| epoch  26 |   600/  745 batches | lr 1.00E-03 | ms/batch 455.17 | loss  4.43 | ppl    83.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 390.08s | valid loss  4.48 | valid ppl    87.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   200/  745 batches | lr 1.00E-03 | ms/batch 455.86 | loss  4.47 | ppl    87.57\n",
      "| epoch  27 |   400/  745 batches | lr 1.00E-03 | ms/batch 461.90 | loss  4.46 | ppl    86.28\n",
      "| epoch  27 |   600/  745 batches | lr 1.00E-03 | ms/batch 457.12 | loss  4.41 | ppl    82.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 390.82s | valid loss  4.47 | valid ppl    87.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   200/  745 batches | lr 1.00E-03 | ms/batch 460.08 | loss  4.47 | ppl    87.03\n",
      "| epoch  28 |   400/  745 batches | lr 1.00E-03 | ms/batch 464.24 | loss  4.45 | ppl    85.31\n",
      "| epoch  28 |   600/  745 batches | lr 1.00E-03 | ms/batch 458.86 | loss  4.40 | ppl    81.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 391.79s | valid loss  4.47 | valid ppl    87.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |   200/  745 batches | lr 1.00E-03 | ms/batch 467.26 | loss  4.45 | ppl    85.26\n",
      "| epoch  29 |   400/  745 batches | lr 1.00E-03 | ms/batch 458.23 | loss  4.44 | ppl    84.44\n",
      "| epoch  29 |   600/  745 batches | lr 1.00E-03 | ms/batch 470.15 | loss  4.38 | ppl    79.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 391.69s | valid loss  4.46 | valid ppl    86.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |   200/  745 batches | lr 1.00E-03 | ms/batch 464.27 | loss  4.43 | ppl    84.00\n",
      "| epoch  30 |   400/  745 batches | lr 1.00E-03 | ms/batch 457.62 | loss  4.40 | ppl    81.85\n",
      "| epoch  30 |   600/  745 batches | lr 1.00E-03 | ms/batch 451.51 | loss  4.37 | ppl    79.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 389.08s | valid loss  4.45 | valid ppl    85.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |   200/  745 batches | lr 1.00E-03 | ms/batch 458.18 | loss  4.42 | ppl    83.14\n",
      "| epoch  31 |   400/  745 batches | lr 1.00E-03 | ms/batch 466.07 | loss  4.40 | ppl    81.54\n",
      "| epoch  31 |   600/  745 batches | lr 1.00E-03 | ms/batch 455.72 | loss  4.35 | ppl    77.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 390.11s | valid loss  4.45 | valid ppl    85.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |   200/  745 batches | lr 1.00E-03 | ms/batch 465.32 | loss  4.41 | ppl    82.29\n",
      "| epoch  32 |   400/  745 batches | lr 1.00E-03 | ms/batch 457.34 | loss  4.39 | ppl    80.33\n",
      "| epoch  32 |   600/  745 batches | lr 1.00E-03 | ms/batch 472.29 | loss  4.34 | ppl    76.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 392.64s | valid loss  4.45 | valid ppl    85.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |   200/  745 batches | lr 1.00E-03 | ms/batch 469.37 | loss  4.40 | ppl    81.24\n",
      "| epoch  33 |   400/  745 batches | lr 1.00E-03 | ms/batch 459.16 | loss  4.38 | ppl    80.17\n",
      "| epoch  33 |   600/  745 batches | lr 1.00E-03 | ms/batch 459.52 | loss  4.33 | ppl    76.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 392.44s | valid loss  4.44 | valid ppl    84.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |   200/  745 batches | lr 1.00E-03 | ms/batch 464.45 | loss  4.38 | ppl    79.53\n",
      "| epoch  34 |   400/  745 batches | lr 1.00E-03 | ms/batch 468.01 | loss  4.37 | ppl    78.94\n",
      "| epoch  34 |   600/  745 batches | lr 1.00E-03 | ms/batch 461.18 | loss  4.33 | ppl    75.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 395.01s | valid loss  4.44 | valid ppl    84.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |   200/  745 batches | lr 1.00E-03 | ms/batch 446.59 | loss  4.38 | ppl    79.51\n",
      "| epoch  35 |   400/  745 batches | lr 1.00E-03 | ms/batch 461.21 | loss  4.35 | ppl    77.70\n",
      "| epoch  35 |   600/  745 batches | lr 1.00E-03 | ms/batch 449.52 | loss  4.33 | ppl    75.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 393.57s | valid loss  4.43 | valid ppl    83.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |   200/  745 batches | lr 1.00E-03 | ms/batch 469.67 | loss  4.36 | ppl    78.53\n",
      "| epoch  36 |   400/  745 batches | lr 1.00E-03 | ms/batch 471.77 | loss  4.35 | ppl    77.34\n",
      "| epoch  36 |   600/  745 batches | lr 1.00E-03 | ms/batch 462.68 | loss  4.30 | ppl    73.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 393.95s | valid loss  4.42 | valid ppl    83.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |   200/  745 batches | lr 1.00E-03 | ms/batch 467.66 | loss  4.36 | ppl    78.00\n",
      "| epoch  37 |   400/  745 batches | lr 1.00E-03 | ms/batch 459.35 | loss  4.34 | ppl    76.86\n",
      "| epoch  37 |   600/  745 batches | lr 1.00E-03 | ms/batch 480.33 | loss  4.29 | ppl    72.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 394.05s | valid loss  4.42 | valid ppl    83.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |   200/  745 batches | lr 1.00E-03 | ms/batch 466.08 | loss  4.34 | ppl    76.88\n",
      "| epoch  38 |   400/  745 batches | lr 1.00E-03 | ms/batch 466.20 | loss  4.33 | ppl    75.92\n",
      "| epoch  38 |   600/  745 batches | lr 1.00E-03 | ms/batch 467.48 | loss  4.29 | ppl    72.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 392.94s | valid loss  4.42 | valid ppl    83.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |   200/  745 batches | lr 1.00E-03 | ms/batch 470.03 | loss  4.34 | ppl    76.76\n",
      "| epoch  39 |   400/  745 batches | lr 1.00E-03 | ms/batch 463.19 | loss  4.32 | ppl    74.95\n",
      "| epoch  39 |   600/  745 batches | lr 1.00E-03 | ms/batch 467.62 | loss  4.26 | ppl    71.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 392.17s | valid loss  4.42 | valid ppl    82.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |   200/  745 batches | lr 1.00E-03 | ms/batch 467.79 | loss  4.32 | ppl    75.15\n",
      "| epoch  40 |   400/  745 batches | lr 1.00E-03 | ms/batch 481.08 | loss  4.31 | ppl    74.11\n",
      "| epoch  40 |   600/  745 batches | lr 1.00E-03 | ms/batch 455.69 | loss  4.26 | ppl    70.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 392.77s | valid loss  4.41 | valid ppl    82.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |   200/  745 batches | lr 1.00E-03 | ms/batch 464.27 | loss  4.31 | ppl    74.76\n",
      "| epoch  41 |   400/  745 batches | lr 1.00E-03 | ms/batch 460.39 | loss  4.30 | ppl    73.52\n",
      "| epoch  41 |   600/  745 batches | lr 1.00E-03 | ms/batch 452.38 | loss  4.25 | ppl    70.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 391.91s | valid loss  4.40 | valid ppl    81.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |   200/  745 batches | lr 1.00E-03 | ms/batch 460.36 | loss  4.31 | ppl    74.66\n",
      "| epoch  42 |   400/  745 batches | lr 1.00E-03 | ms/batch 456.06 | loss  4.29 | ppl    73.01\n",
      "| epoch  42 |   600/  745 batches | lr 1.00E-03 | ms/batch 463.88 | loss  4.25 | ppl    70.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 392.29s | valid loss  4.41 | valid ppl    82.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |   200/  745 batches | lr 5.00E-04 | ms/batch 459.25 | loss  4.29 | ppl    72.98\n",
      "| epoch  43 |   400/  745 batches | lr 5.00E-04 | ms/batch 462.51 | loss  4.26 | ppl    71.04\n",
      "| epoch  43 |   600/  745 batches | lr 5.00E-04 | ms/batch 458.09 | loss  4.20 | ppl    66.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 391.01s | valid loss  4.39 | valid ppl    80.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |   200/  745 batches | lr 5.00E-04 | ms/batch 470.31 | loss  4.27 | ppl    71.29\n",
      "| epoch  44 |   400/  745 batches | lr 5.00E-04 | ms/batch 460.33 | loss  4.24 | ppl    69.50\n",
      "| epoch  44 |   600/  745 batches | lr 5.00E-04 | ms/batch 469.35 | loss  4.18 | ppl    65.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 391.98s | valid loss  4.38 | valid ppl    80.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |   200/  745 batches | lr 5.00E-04 | ms/batch 463.63 | loss  4.25 | ppl    70.15\n",
      "| epoch  45 |   400/  745 batches | lr 5.00E-04 | ms/batch 472.30 | loss  4.23 | ppl    69.00\n",
      "| epoch  45 |   600/  745 batches | lr 5.00E-04 | ms/batch 471.09 | loss  4.17 | ppl    64.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 392.99s | valid loss  4.38 | valid ppl    79.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |   200/  745 batches | lr 5.00E-04 | ms/batch 454.06 | loss  4.24 | ppl    69.14\n",
      "| epoch  46 |   400/  745 batches | lr 5.00E-04 | ms/batch 452.97 | loss  4.22 | ppl    67.71\n",
      "| epoch  46 |   600/  745 batches | lr 5.00E-04 | ms/batch 469.49 | loss  4.17 | ppl    64.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 392.85s | valid loss  4.38 | valid ppl    79.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |   200/  745 batches | lr 5.00E-04 | ms/batch 464.65 | loss  4.22 | ppl    67.80\n",
      "| epoch  47 |   400/  745 batches | lr 5.00E-04 | ms/batch 460.96 | loss  4.21 | ppl    67.25\n",
      "| epoch  47 |   600/  745 batches | lr 5.00E-04 | ms/batch 453.12 | loss  4.16 | ppl    64.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 393.00s | valid loss  4.38 | valid ppl    79.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |   200/  745 batches | lr 5.00E-04 | ms/batch 460.05 | loss  4.23 | ppl    68.45\n",
      "| epoch  48 |   400/  745 batches | lr 5.00E-04 | ms/batch 466.35 | loss  4.20 | ppl    66.96\n",
      "| epoch  48 |   600/  745 batches | lr 5.00E-04 | ms/batch 463.26 | loss  4.14 | ppl    63.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 392.58s | valid loss  4.38 | valid ppl    79.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |   200/  745 batches | lr 5.00E-04 | ms/batch 464.28 | loss  4.21 | ppl    67.57\n",
      "| epoch  49 |   400/  745 batches | lr 5.00E-04 | ms/batch 465.56 | loss  4.19 | ppl    65.96\n",
      "| epoch  49 |   600/  745 batches | lr 5.00E-04 | ms/batch 470.29 | loss  4.14 | ppl    62.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 392.79s | valid loss  4.37 | valid ppl    79.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |   200/  745 batches | lr 5.00E-04 | ms/batch 460.86 | loss  4.20 | ppl    66.98\n",
      "| epoch  50 |   400/  745 batches | lr 5.00E-04 | ms/batch 441.32 | loss  4.17 | ppl    64.82\n",
      "| epoch  50 |   600/  745 batches | lr 5.00E-04 | ms/batch 460.31 | loss  4.15 | ppl    63.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 391.51s | valid loss  4.37 | valid ppl    79.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 |   200/  745 batches | lr 5.00E-04 | ms/batch 467.52 | loss  4.20 | ppl    66.66\n",
      "| epoch  51 |   400/  745 batches | lr 5.00E-04 | ms/batch 457.89 | loss  4.18 | ppl    65.65\n",
      "| epoch  51 |   600/  745 batches | lr 5.00E-04 | ms/batch 463.52 | loss  4.13 | ppl    61.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 391.31s | valid loss  4.37 | valid ppl    79.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 |   200/  745 batches | lr 2.50E-04 | ms/batch 470.09 | loss  4.19 | ppl    66.03\n",
      "| epoch  52 |   400/  745 batches | lr 2.50E-04 | ms/batch 456.82 | loss  4.16 | ppl    64.24\n",
      "| epoch  52 |   600/  745 batches | lr 2.50E-04 | ms/batch 464.37 | loss  4.10 | ppl    60.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 392.42s | valid loss  4.36 | valid ppl    78.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 |   200/  745 batches | lr 2.50E-04 | ms/batch 454.88 | loss  4.18 | ppl    65.08\n",
      "| epoch  53 |   400/  745 batches | lr 2.50E-04 | ms/batch 466.42 | loss  4.15 | ppl    63.28\n",
      "| epoch  53 |   600/  745 batches | lr 2.50E-04 | ms/batch 467.67 | loss  4.10 | ppl    60.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 392.30s | valid loss  4.36 | valid ppl    78.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  54 |   200/  745 batches | lr 2.50E-04 | ms/batch 459.09 | loss  4.17 | ppl    64.52\n",
      "| epoch  54 |   400/  745 batches | lr 2.50E-04 | ms/batch 449.65 | loss  4.14 | ppl    62.74\n",
      "| epoch  54 |   600/  745 batches | lr 2.50E-04 | ms/batch 461.54 | loss  4.10 | ppl    60.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 391.92s | valid loss  4.36 | valid ppl    78.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 |   200/  745 batches | lr 2.50E-04 | ms/batch 454.61 | loss  4.16 | ppl    64.39\n",
      "| epoch  55 |   400/  745 batches | lr 2.50E-04 | ms/batch 460.95 | loss  4.13 | ppl    62.24\n",
      "| epoch  55 |   600/  745 batches | lr 2.50E-04 | ms/batch 465.82 | loss  4.08 | ppl    59.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 391.32s | valid loss  4.36 | valid ppl    78.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  56 |   200/  745 batches | lr 2.50E-04 | ms/batch 461.83 | loss  4.15 | ppl    63.64\n",
      "| epoch  56 |   400/  745 batches | lr 2.50E-04 | ms/batch 458.01 | loss  4.12 | ppl    61.58\n",
      "| epoch  56 |   600/  745 batches | lr 2.50E-04 | ms/batch 461.27 | loss  4.08 | ppl    59.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 393.28s | valid loss  4.36 | valid ppl    77.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  57 |   200/  745 batches | lr 2.50E-04 | ms/batch 473.15 | loss  4.15 | ppl    63.72\n",
      "| epoch  57 |   400/  745 batches | lr 2.50E-04 | ms/batch 465.70 | loss  4.12 | ppl    61.63\n",
      "| epoch  57 |   600/  745 batches | lr 2.50E-04 | ms/batch 452.25 | loss  4.07 | ppl    58.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 391.56s | valid loss  4.36 | valid ppl    77.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  58 |   200/  745 batches | lr 1.25E-04 | ms/batch 446.43 | loss  4.16 | ppl    63.84\n",
      "| epoch  58 |   400/  745 batches | lr 1.25E-04 | ms/batch 464.53 | loss  4.12 | ppl    61.57\n",
      "| epoch  58 |   600/  745 batches | lr 1.25E-04 | ms/batch 462.97 | loss  4.06 | ppl    57.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 391.76s | valid loss  4.35 | valid ppl    77.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  59 |   200/  745 batches | lr 1.25E-04 | ms/batch 456.56 | loss  4.13 | ppl    62.38\n",
      "| epoch  59 |   400/  745 batches | lr 1.25E-04 | ms/batch 466.94 | loss  4.11 | ppl    61.17\n",
      "| epoch  59 |   600/  745 batches | lr 1.25E-04 | ms/batch 459.58 | loss  4.05 | ppl    57.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 391.93s | valid loss  4.35 | valid ppl    77.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 |   200/  745 batches | lr 1.25E-04 | ms/batch 468.39 | loss  4.14 | ppl    62.57\n",
      "| epoch  60 |   400/  745 batches | lr 1.25E-04 | ms/batch 462.36 | loss  4.11 | ppl    60.67\n",
      "| epoch  60 |   600/  745 batches | lr 1.25E-04 | ms/batch 460.76 | loss  4.05 | ppl    57.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 391.50s | valid loss  4.35 | valid ppl    77.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  61 |   200/  745 batches | lr 1.25E-04 | ms/batch 464.15 | loss  4.13 | ppl    62.37\n",
      "| epoch  61 |   400/  745 batches | lr 1.25E-04 | ms/batch 451.69 | loss  4.10 | ppl    60.48\n",
      "| epoch  61 |   600/  745 batches | lr 1.25E-04 | ms/batch 462.31 | loss  4.06 | ppl    57.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 393.61s | valid loss  4.35 | valid ppl    77.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  62 |   200/  745 batches | lr 1.25E-04 | ms/batch 462.63 | loss  4.13 | ppl    62.23\n",
      "| epoch  62 |   400/  745 batches | lr 1.25E-04 | ms/batch 464.15 | loss  4.10 | ppl    60.35\n",
      "| epoch  62 |   600/  745 batches | lr 1.25E-04 | ms/batch 457.44 | loss  4.05 | ppl    57.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 392.97s | valid loss  4.35 | valid ppl    77.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  63 |   200/  745 batches | lr 1.25E-04 | ms/batch 472.24 | loss  4.12 | ppl    61.80\n",
      "| epoch  63 |   400/  745 batches | lr 1.25E-04 | ms/batch 466.01 | loss  4.10 | ppl    60.29\n",
      "| epoch  63 |   600/  745 batches | lr 1.25E-04 | ms/batch 471.69 | loss  4.05 | ppl    57.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 393.15s | valid loss  4.35 | valid ppl    77.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 |   200/  745 batches | lr 6.25E-05 | ms/batch 461.41 | loss  4.12 | ppl    61.75\n",
      "| epoch  64 |   400/  745 batches | lr 6.25E-05 | ms/batch 450.10 | loss  4.10 | ppl    60.17\n",
      "| epoch  64 |   600/  745 batches | lr 6.25E-05 | ms/batch 465.14 | loss  4.05 | ppl    57.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 393.58s | valid loss  4.35 | valid ppl    77.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  65 |   200/  745 batches | lr 6.25E-05 | ms/batch 460.05 | loss  4.12 | ppl    61.78\n",
      "| epoch  65 |   400/  745 batches | lr 6.25E-05 | ms/batch 464.80 | loss  4.10 | ppl    60.08\n",
      "| epoch  65 |   600/  745 batches | lr 6.25E-05 | ms/batch 451.58 | loss  4.05 | ppl    57.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 392.96s | valid loss  4.35 | valid ppl    77.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 |   200/  745 batches | lr 6.25E-05 | ms/batch 461.39 | loss  4.12 | ppl    61.76\n",
      "| epoch  66 |   400/  745 batches | lr 6.25E-05 | ms/batch 462.35 | loss  4.09 | ppl    59.86\n",
      "| epoch  66 |   600/  745 batches | lr 6.25E-05 | ms/batch 459.52 | loss  4.04 | ppl    56.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 394.04s | valid loss  4.35 | valid ppl    77.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 |   200/  745 batches | lr 6.25E-05 | ms/batch 463.74 | loss  4.10 | ppl    60.42\n",
      "| epoch  67 |   400/  745 batches | lr 6.25E-05 | ms/batch 459.64 | loss  4.09 | ppl    59.97\n",
      "| epoch  67 |   600/  745 batches | lr 6.25E-05 | ms/batch 456.37 | loss  4.03 | ppl    56.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 389.85s | valid loss  4.35 | valid ppl    77.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  68 |   200/  745 batches | lr 6.25E-05 | ms/batch 471.34 | loss  4.11 | ppl    61.23\n",
      "| epoch  68 |   400/  745 batches | lr 6.25E-05 | ms/batch 457.78 | loss  4.10 | ppl    60.33\n",
      "| epoch  68 |   600/  745 batches | lr 6.25E-05 | ms/batch 461.87 | loss  4.03 | ppl    56.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 393.27s | valid loss  4.35 | valid ppl    77.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  69 |   200/  745 batches | lr 6.25E-05 | ms/batch 454.42 | loss  4.12 | ppl    61.55\n",
      "| epoch  69 |   400/  745 batches | lr 6.25E-05 | ms/batch 465.94 | loss  4.08 | ppl    59.26\n",
      "| epoch  69 |   600/  745 batches | lr 6.25E-05 | ms/batch 465.19 | loss  4.03 | ppl    56.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 392.56s | valid loss  4.35 | valid ppl    77.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  70 |   200/  745 batches | lr 6.25E-05 | ms/batch 471.63 | loss  4.11 | ppl    60.87\n",
      "| epoch  70 |   400/  745 batches | lr 6.25E-05 | ms/batch 465.14 | loss  4.09 | ppl    59.68\n",
      "| epoch  70 |   600/  745 batches | lr 6.25E-05 | ms/batch 462.41 | loss  4.03 | ppl    56.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 393.09s | valid loss  4.35 | valid ppl    77.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = 1e20\n",
    "try:\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(valid_data, args.eval_batch_size)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "            epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        if val_loss < best_val_loss:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            lr *= LR_ANNEALING_RATE\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "with open(args.save, 'rb') as f:\n",
    "    model = torch.load(f, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  4.29 | test ppl    72.90\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss = evaluate(test_data, args.test_batch_size)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
