{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import WD_LSTM\n",
    "from data import Corpus\n",
    "from randomize_bptt import get_bptt_sequence_lengths\n",
    "from helpers import Config, repackage_hidden, batchify, get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "DATA = '/floyd/input/ptb/'\n",
    "CUDA = True\n",
    "LOG_INTERVAL = 100\n",
    "LR_ANNEALING_RATE = 0.25\n",
    "CONFIG_NAME = 'language_model_base'\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
    "args = Config(CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "train_data = batchify(corpus.train, args.batch_size, device)\n",
    "valid_data = batchify(corpus.valid, args.batch_size, device)\n",
    "test_data = batchify(corpus.test, args.batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WD_LSTM(\n",
       "  (drop): Dropout(p=0.4)\n",
       "  (variational_dropout): VariationalDropout()\n",
       "  (encoder): Embedding(10000, 400)\n",
       "  (rnns): ModuleList(\n",
       "    (0): WeightDrop(\n",
       "      (module): LSTM(400, 1150)\n",
       "    )\n",
       "    (1): WeightDrop(\n",
       "      (module): LSTM(1150, 1150)\n",
       "    )\n",
       "    (2): WeightDrop(\n",
       "      (module): LSTM(1150, 400)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=400, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WD_LSTM(\n",
    "    ntokens, \n",
    "    args.emsize,\n",
    "    args.nhid, \n",
    "    args.nlayers, \n",
    "    args.dropout,\n",
    "    args.dropout_h,\n",
    "    weight_drop=args.weight_drop, \n",
    "    weight_tying=args.weight_tying\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = args.lr\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=args.weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    model.eval()  # disable dropout\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args.batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, args.bptt_seq_len):\n",
    "            data, targets = get_batch(data_source, i, args.bptt_seq_len)\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args.batch_size)\n",
    "    for batch, (i, seq_len, lr_scale) in enumerate(get_bptt_sequence_lengths(\n",
    "        train_data.size(0), \n",
    "        args.bptt_seq_len, \n",
    "        args.bptt_random_scaling, \n",
    "        args.bptt_p, \n",
    "        args.bptt_s, \n",
    "        args.bptt_min_len, \n",
    "        args.bptt_max_len\n",
    "    )):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr * lr_scale\n",
    "        data, targets = get_batch(train_data, i, seq_len)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
    "            cur_loss = total_loss / LOG_INTERVAL\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:3.2E} | ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args.bptt_seq_len, lr,\n",
    "                elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py:477: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/  331 batches | lr 2.00E-03 | ms/batch 367.19 | loss  6.84 | ppl   938.29\n",
      "| epoch   1 |   200/  331 batches | lr 2.00E-03 | ms/batch 376.05 | loss  6.38 | ppl   590.19\n",
      "| epoch   1 |   300/  331 batches | lr 2.00E-03 | ms/batch 366.28 | loss  6.07 | ppl   432.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 129.50s | valid loss  5.75 | valid ppl   312.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   100/  331 batches | lr 2.00E-03 | ms/batch 373.66 | loss  5.87 | ppl   354.27\n",
      "| epoch   2 |   200/  331 batches | lr 2.00E-03 | ms/batch 376.37 | loss  5.76 | ppl   316.29\n",
      "| epoch   2 |   300/  331 batches | lr 2.00E-03 | ms/batch 371.43 | loss  5.63 | ppl   277.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 130.50s | valid loss  5.39 | valid ppl   219.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   100/  331 batches | lr 2.00E-03 | ms/batch 371.77 | loss  5.57 | ppl   263.47\n",
      "| epoch   3 |   200/  331 batches | lr 2.00E-03 | ms/batch 375.29 | loss  5.51 | ppl   248.31\n",
      "| epoch   3 |   300/  331 batches | lr 2.00E-03 | ms/batch 377.96 | loss  5.43 | ppl   228.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 131.09s | valid loss  5.21 | valid ppl   182.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   100/  331 batches | lr 2.00E-03 | ms/batch 373.29 | loss  5.41 | ppl   224.72\n",
      "| epoch   4 |   200/  331 batches | lr 2.00E-03 | ms/batch 366.59 | loss  5.38 | ppl   215.99\n",
      "| epoch   4 |   300/  331 batches | lr 2.00E-03 | ms/batch 368.22 | loss  5.30 | ppl   201.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 131.15s | valid loss  5.09 | valid ppl   162.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   100/  331 batches | lr 2.00E-03 | ms/batch 379.96 | loss  5.30 | ppl   200.92\n",
      "| epoch   5 |   200/  331 batches | lr 2.00E-03 | ms/batch 372.57 | loss  5.30 | ppl   199.47\n",
      "| epoch   5 |   300/  331 batches | lr 2.00E-03 | ms/batch 373.94 | loss  5.21 | ppl   182.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 130.89s | valid loss  5.01 | valid ppl   150.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   100/  331 batches | lr 2.00E-03 | ms/batch 370.79 | loss  5.23 | ppl   186.98\n",
      "| epoch   6 |   200/  331 batches | lr 2.00E-03 | ms/batch 380.38 | loss  5.21 | ppl   183.61\n",
      "| epoch   6 |   300/  331 batches | lr 2.00E-03 | ms/batch 370.61 | loss  5.14 | ppl   170.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 131.10s | valid loss  4.96 | valid ppl   143.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   100/  331 batches | lr 2.00E-03 | ms/batch 365.04 | loss  5.17 | ppl   176.08\n",
      "| epoch   7 |   200/  331 batches | lr 2.00E-03 | ms/batch 375.92 | loss  5.16 | ppl   174.20\n",
      "| epoch   7 |   300/  331 batches | lr 2.00E-03 | ms/batch 370.39 | loss  5.09 | ppl   161.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 131.31s | valid loss  4.90 | valid ppl   134.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   100/  331 batches | lr 2.00E-03 | ms/batch 379.45 | loss  5.11 | ppl   166.04\n",
      "| epoch   8 |   200/  331 batches | lr 2.00E-03 | ms/batch 374.89 | loss  5.12 | ppl   166.81\n",
      "| epoch   8 |   300/  331 batches | lr 2.00E-03 | ms/batch 373.04 | loss  5.04 | ppl   154.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 131.01s | valid loss  4.86 | valid ppl   129.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   100/  331 batches | lr 2.00E-03 | ms/batch 365.18 | loss  5.06 | ppl   158.16\n",
      "| epoch   9 |   200/  331 batches | lr 2.00E-03 | ms/batch 381.56 | loss  5.06 | ppl   158.04\n",
      "| epoch   9 |   300/  331 batches | lr 2.00E-03 | ms/batch 369.71 | loss  4.99 | ppl   146.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 131.32s | valid loss  4.82 | valid ppl   123.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   100/  331 batches | lr 2.00E-03 | ms/batch 379.22 | loss  5.03 | ppl   152.92\n",
      "| epoch  10 |   200/  331 batches | lr 2.00E-03 | ms/batch 363.64 | loss  5.03 | ppl   153.09\n",
      "| epoch  10 |   300/  331 batches | lr 2.00E-03 | ms/batch 371.25 | loss  4.96 | ppl   142.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 131.32s | valid loss  4.80 | valid ppl   121.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   100/  331 batches | lr 2.00E-03 | ms/batch 382.20 | loss  5.01 | ppl   149.23\n",
      "| epoch  11 |   200/  331 batches | lr 2.00E-03 | ms/batch 375.32 | loss  4.99 | ppl   147.41\n",
      "| epoch  11 |   300/  331 batches | lr 2.00E-03 | ms/batch 373.60 | loss  4.94 | ppl   139.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 131.17s | valid loss  4.78 | valid ppl   118.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   100/  331 batches | lr 2.00E-03 | ms/batch 365.89 | loss  4.97 | ppl   144.38\n",
      "| epoch  12 |   200/  331 batches | lr 2.00E-03 | ms/batch 371.30 | loss  4.97 | ppl   144.68\n",
      "| epoch  12 |   300/  331 batches | lr 2.00E-03 | ms/batch 377.82 | loss  4.90 | ppl   134.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 131.40s | valid loss  4.75 | valid ppl   115.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   100/  331 batches | lr 2.00E-03 | ms/batch 376.19 | loss  4.94 | ppl   139.76\n",
      "| epoch  13 |   200/  331 batches | lr 2.00E-03 | ms/batch 369.19 | loss  4.95 | ppl   141.87\n",
      "| epoch  13 |   300/  331 batches | lr 2.00E-03 | ms/batch 373.09 | loss  4.88 | ppl   131.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 131.51s | valid loss  4.74 | valid ppl   114.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   100/  331 batches | lr 2.00E-03 | ms/batch 380.40 | loss  4.93 | ppl   137.82\n",
      "| epoch  14 |   200/  331 batches | lr 2.00E-03 | ms/batch 365.25 | loss  4.94 | ppl   139.20\n",
      "| epoch  14 |   300/  331 batches | lr 2.00E-03 | ms/batch 367.90 | loss  4.87 | ppl   130.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 131.29s | valid loss  4.71 | valid ppl   111.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   100/  331 batches | lr 2.00E-03 | ms/batch 372.98 | loss  4.92 | ppl   136.72\n",
      "| epoch  15 |   200/  331 batches | lr 2.00E-03 | ms/batch 380.16 | loss  4.92 | ppl   136.70\n",
      "| epoch  15 |   300/  331 batches | lr 2.00E-03 | ms/batch 378.04 | loss  4.85 | ppl   128.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 131.49s | valid loss  4.70 | valid ppl   109.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   100/  331 batches | lr 2.00E-03 | ms/batch 374.84 | loss  4.91 | ppl   135.79\n",
      "| epoch  16 |   200/  331 batches | lr 2.00E-03 | ms/batch 378.20 | loss  4.90 | ppl   134.93\n",
      "| epoch  16 |   300/  331 batches | lr 2.00E-03 | ms/batch 376.42 | loss  4.83 | ppl   124.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 131.47s | valid loss  4.69 | valid ppl   109.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   100/  331 batches | lr 2.00E-03 | ms/batch 367.89 | loss  4.88 | ppl   131.51\n",
      "| epoch  17 |   200/  331 batches | lr 2.00E-03 | ms/batch 370.27 | loss  4.89 | ppl   133.55\n",
      "| epoch  17 |   300/  331 batches | lr 2.00E-03 | ms/batch 374.49 | loss  4.82 | ppl   123.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 131.78s | valid loss  4.68 | valid ppl   107.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   100/  331 batches | lr 2.00E-03 | ms/batch 380.46 | loss  4.88 | ppl   131.46\n",
      "| epoch  18 |   200/  331 batches | lr 2.00E-03 | ms/batch 373.84 | loss  4.89 | ppl   132.85\n",
      "| epoch  18 |   300/  331 batches | lr 2.00E-03 | ms/batch 377.42 | loss  4.81 | ppl   123.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 131.63s | valid loss  4.68 | valid ppl   107.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   100/  331 batches | lr 2.00E-03 | ms/batch 373.55 | loss  4.86 | ppl   129.43\n",
      "| epoch  19 |   200/  331 batches | lr 2.00E-03 | ms/batch 375.79 | loss  4.88 | ppl   131.09\n",
      "| epoch  19 |   300/  331 batches | lr 2.00E-03 | ms/batch 365.85 | loss  4.79 | ppl   120.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 131.72s | valid loss  4.66 | valid ppl   106.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   100/  331 batches | lr 2.00E-03 | ms/batch 376.81 | loss  4.84 | ppl   127.05\n",
      "| epoch  20 |   200/  331 batches | lr 2.00E-03 | ms/batch 375.45 | loss  4.87 | ppl   130.10\n",
      "| epoch  20 |   300/  331 batches | lr 2.00E-03 | ms/batch 364.74 | loss  4.79 | ppl   119.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 131.67s | valid loss  4.66 | valid ppl   105.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   100/  331 batches | lr 2.00E-03 | ms/batch 381.92 | loss  4.85 | ppl   127.11\n",
      "| epoch  21 |   200/  331 batches | lr 2.00E-03 | ms/batch 374.62 | loss  4.85 | ppl   128.20\n",
      "| epoch  21 |   300/  331 batches | lr 2.00E-03 | ms/batch 375.76 | loss  4.79 | ppl   120.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 131.54s | valid loss  4.64 | valid ppl   104.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   100/  331 batches | lr 2.00E-03 | ms/batch 369.51 | loss  4.83 | ppl   125.62\n",
      "| epoch  22 |   200/  331 batches | lr 2.00E-03 | ms/batch 372.47 | loss  4.84 | ppl   126.39\n",
      "| epoch  22 |   300/  331 batches | lr 2.00E-03 | ms/batch 378.56 | loss  4.77 | ppl   118.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 131.50s | valid loss  4.64 | valid ppl   103.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   100/  331 batches | lr 2.00E-03 | ms/batch 372.36 | loss  4.83 | ppl   125.28\n",
      "| epoch  23 |   200/  331 batches | lr 2.00E-03 | ms/batch 367.88 | loss  4.84 | ppl   127.02\n",
      "| epoch  23 |   300/  331 batches | lr 2.00E-03 | ms/batch 373.09 | loss  4.77 | ppl   117.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 131.54s | valid loss  4.63 | valid ppl   102.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   100/  331 batches | lr 2.00E-03 | ms/batch 378.76 | loss  4.82 | ppl   123.89\n",
      "| epoch  24 |   200/  331 batches | lr 2.00E-03 | ms/batch 372.64 | loss  4.83 | ppl   125.48\n",
      "| epoch  24 |   300/  331 batches | lr 2.00E-03 | ms/batch 371.19 | loss  4.75 | ppl   115.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 131.72s | valid loss  4.62 | valid ppl   101.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |   100/  331 batches | lr 2.00E-03 | ms/batch 377.76 | loss  4.82 | ppl   123.92\n",
      "| epoch  25 |   200/  331 batches | lr 2.00E-03 | ms/batch 377.57 | loss  4.81 | ppl   122.48\n",
      "| epoch  25 |   300/  331 batches | lr 2.00E-03 | ms/batch 375.60 | loss  4.75 | ppl   115.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 131.49s | valid loss  4.62 | valid ppl   101.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   100/  331 batches | lr 5.00E-04 | ms/batch 385.76 | loss  4.78 | ppl   119.17\n",
      "| epoch  26 |   200/  331 batches | lr 5.00E-04 | ms/batch 367.31 | loss  4.73 | ppl   113.35\n",
      "| epoch  26 |   300/  331 batches | lr 5.00E-04 | ms/batch 375.41 | loss  4.61 | ppl   100.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 131.66s | valid loss  4.56 | valid ppl    95.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   100/  331 batches | lr 5.00E-04 | ms/batch 367.95 | loss  4.71 | ppl   111.09\n",
      "| epoch  27 |   200/  331 batches | lr 5.00E-04 | ms/batch 378.77 | loss  4.71 | ppl   110.95\n",
      "| epoch  27 |   300/  331 batches | lr 5.00E-04 | ms/batch 376.18 | loss  4.58 | ppl    97.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 131.54s | valid loss  4.54 | valid ppl    93.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   100/  331 batches | lr 5.00E-04 | ms/batch 374.24 | loss  4.68 | ppl   108.01\n",
      "| epoch  28 |   200/  331 batches | lr 5.00E-04 | ms/batch 373.36 | loss  4.69 | ppl   108.48\n",
      "| epoch  28 |   300/  331 batches | lr 5.00E-04 | ms/batch 371.64 | loss  4.57 | ppl    96.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 131.52s | valid loss  4.53 | valid ppl    93.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |   100/  331 batches | lr 5.00E-04 | ms/batch 382.53 | loss  4.66 | ppl   105.81\n",
      "| epoch  29 |   200/  331 batches | lr 5.00E-04 | ms/batch 367.43 | loss  4.66 | ppl   105.90\n",
      "| epoch  29 |   300/  331 batches | lr 5.00E-04 | ms/batch 378.24 | loss  4.57 | ppl    96.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 131.51s | valid loss  4.53 | valid ppl    92.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |   100/  331 batches | lr 5.00E-04 | ms/batch 373.73 | loss  4.65 | ppl   104.52\n",
      "| epoch  30 |   200/  331 batches | lr 5.00E-04 | ms/batch 374.89 | loss  4.65 | ppl   104.35\n",
      "| epoch  30 |   300/  331 batches | lr 5.00E-04 | ms/batch 377.27 | loss  4.56 | ppl    95.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 131.60s | valid loss  4.53 | valid ppl    92.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |   100/  331 batches | lr 5.00E-04 | ms/batch 383.30 | loss  4.64 | ppl   103.35\n",
      "| epoch  31 |   200/  331 batches | lr 5.00E-04 | ms/batch 377.18 | loss  4.64 | ppl   103.51\n",
      "| epoch  31 |   300/  331 batches | lr 5.00E-04 | ms/batch 363.77 | loss  4.53 | ppl    93.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 131.58s | valid loss  4.52 | valid ppl    92.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |   100/  331 batches | lr 5.00E-04 | ms/batch 390.11 | loss  4.64 | ppl   103.54\n",
      "| epoch  32 |   200/  331 batches | lr 5.00E-04 | ms/batch 370.02 | loss  4.64 | ppl   103.53\n",
      "| epoch  32 |   300/  331 batches | lr 5.00E-04 | ms/batch 371.86 | loss  4.54 | ppl    94.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 132.54s | valid loss  4.52 | valid ppl    91.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |   100/  331 batches | lr 5.00E-04 | ms/batch 377.41 | loss  4.63 | ppl   102.31\n",
      "| epoch  33 |   200/  331 batches | lr 5.00E-04 | ms/batch 369.89 | loss  4.64 | ppl   103.81\n",
      "| epoch  33 |   300/  331 batches | lr 5.00E-04 | ms/batch 371.27 | loss  4.53 | ppl    92.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 132.86s | valid loss  4.52 | valid ppl    91.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |   100/  331 batches | lr 5.00E-04 | ms/batch 374.72 | loss  4.62 | ppl   101.05\n",
      "| epoch  34 |   200/  331 batches | lr 5.00E-04 | ms/batch 375.01 | loss  4.64 | ppl   103.19\n",
      "| epoch  34 |   300/  331 batches | lr 5.00E-04 | ms/batch 373.46 | loss  4.53 | ppl    93.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 132.66s | valid loss  4.52 | valid ppl    91.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |   100/  331 batches | lr 1.25E-04 | ms/batch 381.12 | loss  4.61 | ppl   100.02\n",
      "| epoch  35 |   200/  331 batches | lr 1.25E-04 | ms/batch 371.45 | loss  4.61 | ppl   100.42\n",
      "| epoch  35 |   300/  331 batches | lr 1.25E-04 | ms/batch 366.82 | loss  4.49 | ppl    89.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 132.80s | valid loss  4.50 | valid ppl    90.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |   100/  331 batches | lr 1.25E-04 | ms/batch 385.11 | loss  4.59 | ppl    98.52\n",
      "| epoch  36 |   200/  331 batches | lr 1.25E-04 | ms/batch 384.75 | loss  4.60 | ppl    99.08\n",
      "| epoch  36 |   300/  331 batches | lr 1.25E-04 | ms/batch 377.00 | loss  4.49 | ppl    88.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 132.58s | valid loss  4.50 | valid ppl    89.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |   100/  331 batches | lr 1.25E-04 | ms/batch 379.66 | loss  4.60 | ppl    99.77\n",
      "| epoch  37 |   200/  331 batches | lr 1.25E-04 | ms/batch 384.70 | loss  4.59 | ppl    98.22\n",
      "| epoch  37 |   300/  331 batches | lr 1.25E-04 | ms/batch 379.64 | loss  4.48 | ppl    88.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 132.58s | valid loss  4.49 | valid ppl    89.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |   100/  331 batches | lr 1.25E-04 | ms/batch 379.07 | loss  4.58 | ppl    97.99\n",
      "| epoch  38 |   200/  331 batches | lr 1.25E-04 | ms/batch 375.34 | loss  4.58 | ppl    97.85\n",
      "| epoch  38 |   300/  331 batches | lr 1.25E-04 | ms/batch 375.67 | loss  4.48 | ppl    88.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 132.27s | valid loss  4.49 | valid ppl    89.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |   100/  331 batches | lr 1.25E-04 | ms/batch 388.02 | loss  4.58 | ppl    97.39\n",
      "| epoch  39 |   200/  331 batches | lr 1.25E-04 | ms/batch 383.27 | loss  4.58 | ppl    97.96\n",
      "| epoch  39 |   300/  331 batches | lr 1.25E-04 | ms/batch 383.44 | loss  4.50 | ppl    89.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 132.51s | valid loss  4.49 | valid ppl    89.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |   100/  331 batches | lr 1.25E-04 | ms/batch 373.23 | loss  4.57 | ppl    96.95\n",
      "| epoch  40 |   200/  331 batches | lr 1.25E-04 | ms/batch 374.65 | loss  4.58 | ppl    97.21\n",
      "| epoch  40 |   300/  331 batches | lr 1.25E-04 | ms/batch 374.84 | loss  4.48 | ppl    88.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 132.71s | valid loss  4.49 | valid ppl    89.08\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = 1e20\n",
    "try:\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(valid_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "            epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        if val_loss < best_val_loss:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            lr *= LR_ANNEALING_RATE\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "with open(args.save, 'rb') as f:\n",
    "    model = torch.load(f, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  4.46 | test ppl    86.08\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
