{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import WD_LSTM\n",
    "from data import Corpus\n",
    "from randomize_bptt import get_bptt_sequence_lengths\n",
    "from helpers import Config, repackage_hidden, batchify, get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "DATA = '/floyd/home/sein/'\n",
    "CUDA = True\n",
    "LOG_INTERVAL = 100\n",
    "LR_ANNEALING_RATE = 0.25\n",
    "CONFIG_NAME = 'language_model_base'\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
    "args = Config(CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "train_data = batchify(corpus.train, args.batch_size, device)\n",
    "valid_data = batchify(corpus.valid, args.eval_batch_size, device)\n",
    "test_data = batchify(corpus.test, args.test_batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WD_LSTM(\n",
       "  (variational_dropout): VariationalDropout()\n",
       "  (encoder): Embedding(18818, 400)\n",
       "  (rnns): ModuleList(\n",
       "    (0): WeightDrop(\n",
       "      (module): LSTM(400, 1150)\n",
       "    )\n",
       "    (1): WeightDrop(\n",
       "      (module): LSTM(1150, 1150)\n",
       "    )\n",
       "    (2): WeightDrop(\n",
       "      (module): LSTM(1150, 400)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=400, out_features=18818, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WD_LSTM(\n",
    "    ntoken=ntokens, \n",
    "    ninp=args.emsize,\n",
    "    nhid=args.nhid, \n",
    "    nlayers=args.nlayers, \n",
    "    dropout=args.dropout,\n",
    "    dropout_h=args.dropout_h,\n",
    "    dropout_i=args.dropout_i,\n",
    "    dropout_e=args.dropout_e,\n",
    "    weight_drop=args.weight_drop, \n",
    "    weight_tying=args.weight_tying\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = args.lr\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=args.weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source, batch_size):\n",
    "    model.eval()  # disable dropout\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, args.bptt_seq_len):\n",
    "            data, targets = get_batch(data_source, i, args.bptt_seq_len)\n",
    "            output, hidden, _, _ = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_tar_regularization_loss(outputs, dropped_outputs):\n",
    "    reg_loss = 0\n",
    "    for i in range(len(dropped_outputs[-1])):\n",
    "        reg_loss += args.alpha * dropped_outputs[-1][i].pow(2).mean()\n",
    "        if i >= 1:\n",
    "            reg_loss += args.beta * (outputs[-1][i] - outputs[-1][i - 1]).pow(2).mean()\n",
    "    return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args.batch_size)\n",
    "    for batch, (i, seq_len, lr_scale) in enumerate(get_bptt_sequence_lengths(\n",
    "        train_data.size(0), \n",
    "        args.bptt_seq_len, \n",
    "        args.bptt_random_scaling, \n",
    "        args.bptt_p, \n",
    "        args.bptt_s, \n",
    "        args.bptt_min_len\n",
    "    )):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr * lr_scale\n",
    "        data, targets = get_batch(train_data, i, seq_len)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden, outputs, dropped_outputs = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        unregularized_loss = criterion(output_flat, targets)\n",
    "        regularized_loss = unregularized_loss + ar_tar_regularization_loss(outputs, dropped_outputs)\n",
    "        regularized_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "        total_loss += unregularized_loss.item()\n",
    "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
    "            cur_loss = total_loss / LOG_INTERVAL\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:3.2E} | ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args.bptt_seq_len, lr,\n",
    "                elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py:477: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/  523 batches | lr 4.00E-03 | ms/batch 240.36 | loss  6.28 | ppl   536.46\n",
      "| epoch   1 |   200/  523 batches | lr 4.00E-03 | ms/batch 237.09 | loss  4.94 | ppl   139.34\n",
      "| epoch   1 |   300/  523 batches | lr 4.00E-03 | ms/batch 233.44 | loss  4.68 | ppl   107.30\n",
      "| epoch   1 |   400/  523 batches | lr 4.00E-03 | ms/batch 240.32 | loss  4.57 | ppl    96.29\n",
      "| epoch   1 |   500/  523 batches | lr 4.00E-03 | ms/batch 232.63 | loss  4.46 | ppl    86.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 140.87s | valid loss  4.38 | valid ppl    80.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   100/  523 batches | lr 4.00E-03 | ms/batch 235.62 | loss  4.34 | ppl    77.02\n",
      "| epoch   2 |   200/  523 batches | lr 4.00E-03 | ms/batch 231.45 | loss  4.22 | ppl    67.83\n",
      "| epoch   2 |   300/  523 batches | lr 4.00E-03 | ms/batch 239.65 | loss  4.19 | ppl    66.14\n",
      "| epoch   2 |   400/  523 batches | lr 4.00E-03 | ms/batch 234.17 | loss  4.20 | ppl    66.82\n",
      "| epoch   2 |   500/  523 batches | lr 4.00E-03 | ms/batch 233.53 | loss  4.18 | ppl    65.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 140.78s | valid loss  4.19 | valid ppl    65.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   100/  523 batches | lr 4.00E-03 | ms/batch 238.23 | loss  4.12 | ppl    61.44\n",
      "| epoch   3 |   200/  523 batches | lr 4.00E-03 | ms/batch 232.28 | loss  4.04 | ppl    56.83\n",
      "| epoch   3 |   300/  523 batches | lr 4.00E-03 | ms/batch 232.80 | loss  4.04 | ppl    56.65\n",
      "| epoch   3 |   400/  523 batches | lr 4.00E-03 | ms/batch 236.93 | loss  4.04 | ppl    57.08\n",
      "| epoch   3 |   500/  523 batches | lr 4.00E-03 | ms/batch 230.19 | loss  4.07 | ppl    58.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 140.83s | valid loss  4.10 | valid ppl    60.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   100/  523 batches | lr 4.00E-03 | ms/batch 239.83 | loss  4.01 | ppl    55.04\n",
      "| epoch   4 |   200/  523 batches | lr 4.00E-03 | ms/batch 239.12 | loss  3.92 | ppl    50.55\n",
      "| epoch   4 |   300/  523 batches | lr 4.00E-03 | ms/batch 238.87 | loss  3.92 | ppl    50.44\n",
      "| epoch   4 |   400/  523 batches | lr 4.00E-03 | ms/batch 235.47 | loss  3.97 | ppl    52.87\n",
      "| epoch   4 |   500/  523 batches | lr 4.00E-03 | ms/batch 236.04 | loss  3.96 | ppl    52.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 140.48s | valid loss  4.05 | valid ppl    57.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   100/  523 batches | lr 4.00E-03 | ms/batch 236.82 | loss  3.94 | ppl    51.63\n",
      "| epoch   5 |   200/  523 batches | lr 4.00E-03 | ms/batch 222.63 | loss  3.86 | ppl    47.28\n",
      "| epoch   5 |   300/  523 batches | lr 4.00E-03 | ms/batch 233.07 | loss  3.86 | ppl    47.31\n",
      "| epoch   5 |   400/  523 batches | lr 4.00E-03 | ms/batch 237.30 | loss  3.89 | ppl    48.76\n",
      "| epoch   5 |   500/  523 batches | lr 4.00E-03 | ms/batch 230.22 | loss  3.91 | ppl    49.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 141.06s | valid loss  4.01 | valid ppl    55.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   100/  523 batches | lr 4.00E-03 | ms/batch 235.27 | loss  3.87 | ppl    47.78\n",
      "| epoch   6 |   200/  523 batches | lr 4.00E-03 | ms/batch 241.48 | loss  3.79 | ppl    44.47\n",
      "| epoch   6 |   300/  523 batches | lr 4.00E-03 | ms/batch 231.46 | loss  3.80 | ppl    44.74\n",
      "| epoch   6 |   400/  523 batches | lr 4.00E-03 | ms/batch 238.12 | loss  3.86 | ppl    47.55\n",
      "| epoch   6 |   500/  523 batches | lr 4.00E-03 | ms/batch 230.48 | loss  3.85 | ppl    47.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 140.74s | valid loss  3.99 | valid ppl    54.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   100/  523 batches | lr 4.00E-03 | ms/batch 239.16 | loss  3.84 | ppl    46.43\n",
      "| epoch   7 |   200/  523 batches | lr 4.00E-03 | ms/batch 238.05 | loss  3.74 | ppl    42.24\n",
      "| epoch   7 |   300/  523 batches | lr 4.00E-03 | ms/batch 235.78 | loss  3.76 | ppl    42.77\n",
      "| epoch   7 |   400/  523 batches | lr 4.00E-03 | ms/batch 235.20 | loss  3.82 | ppl    45.56\n",
      "| epoch   7 |   500/  523 batches | lr 4.00E-03 | ms/batch 230.79 | loss  3.80 | ppl    44.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 140.75s | valid loss  3.97 | valid ppl    52.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   100/  523 batches | lr 4.00E-03 | ms/batch 238.60 | loss  3.79 | ppl    44.24\n",
      "| epoch   8 |   200/  523 batches | lr 4.00E-03 | ms/batch 241.62 | loss  3.73 | ppl    41.65\n",
      "| epoch   8 |   300/  523 batches | lr 4.00E-03 | ms/batch 245.18 | loss  3.73 | ppl    41.59\n",
      "| epoch   8 |   400/  523 batches | lr 4.00E-03 | ms/batch 232.87 | loss  3.79 | ppl    44.21\n",
      "| epoch   8 |   500/  523 batches | lr 4.00E-03 | ms/batch 232.98 | loss  3.78 | ppl    43.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 140.63s | valid loss  3.96 | valid ppl    52.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   100/  523 batches | lr 4.00E-03 | ms/batch 240.43 | loss  3.77 | ppl    43.20\n",
      "| epoch   9 |   200/  523 batches | lr 4.00E-03 | ms/batch 233.80 | loss  3.68 | ppl    39.52\n",
      "| epoch   9 |   300/  523 batches | lr 4.00E-03 | ms/batch 240.55 | loss  3.69 | ppl    40.20\n",
      "| epoch   9 |   400/  523 batches | lr 4.00E-03 | ms/batch 235.62 | loss  3.76 | ppl    43.04\n",
      "| epoch   9 |   500/  523 batches | lr 4.00E-03 | ms/batch 239.57 | loss  3.76 | ppl    42.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 140.57s | valid loss  3.95 | valid ppl    52.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   100/  523 batches | lr 4.00E-03 | ms/batch 235.16 | loss  3.74 | ppl    42.14\n",
      "| epoch  10 |   200/  523 batches | lr 4.00E-03 | ms/batch 236.78 | loss  3.67 | ppl    39.41\n",
      "| epoch  10 |   300/  523 batches | lr 4.00E-03 | ms/batch 227.47 | loss  3.67 | ppl    39.22\n",
      "| epoch  10 |   400/  523 batches | lr 4.00E-03 | ms/batch 231.90 | loss  3.72 | ppl    41.28\n",
      "| epoch  10 |   500/  523 batches | lr 4.00E-03 | ms/batch 233.27 | loss  3.76 | ppl    43.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 141.07s | valid loss  3.94 | valid ppl    51.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   100/  523 batches | lr 4.00E-03 | ms/batch 239.47 | loss  3.72 | ppl    41.09\n",
      "| epoch  11 |   200/  523 batches | lr 4.00E-03 | ms/batch 239.12 | loss  3.63 | ppl    37.66\n",
      "| epoch  11 |   300/  523 batches | lr 4.00E-03 | ms/batch 233.19 | loss  3.64 | ppl    38.19\n",
      "| epoch  11 |   400/  523 batches | lr 4.00E-03 | ms/batch 235.75 | loss  3.70 | ppl    40.53\n",
      "| epoch  11 |   500/  523 batches | lr 4.00E-03 | ms/batch 233.39 | loss  3.70 | ppl    40.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 140.62s | valid loss  3.94 | valid ppl    51.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   100/  523 batches | lr 4.00E-03 | ms/batch 240.84 | loss  3.68 | ppl    39.61\n",
      "| epoch  12 |   200/  523 batches | lr 4.00E-03 | ms/batch 230.60 | loss  3.62 | ppl    37.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = 1e20\n",
    "try:\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(valid_data, args.eval_batch_size)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "            epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        if val_loss < best_val_loss:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            lr *= LR_ANNEALING_RATE\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "with open(args.save, 'rb') as f:\n",
    "    model = torch.load(f, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  3.90 | test ppl    49.55\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss = evaluate(test_data, args.test_batch_size)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
